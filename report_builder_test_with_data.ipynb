{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec42149c",
   "metadata": {},
   "source": [
    "## We get imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dc9e2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from reportlab.platypus import BaseDocTemplate, PageTemplate, Frame\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image, Spacer, Paragraph, KeepTogether\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from io import BytesIO\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import textwrap\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from reportlab.platypus import Image, Spacer, Paragraph, KeepTogether\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from io import BytesIO\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from reportlab.platypus import SimpleDocTemplate\n",
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc3fae",
   "metadata": {},
   "source": [
    "## We configure the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58088a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set. True Usable Width is: 456.0\n"
     ]
    }
   ],
   "source": [
    "# 1. File Paths\n",
    "ROBOTO_REGULAR_PATH = '../Report builder/Fonts/Roboto/static/Roboto-Regular.ttf'\n",
    "ROBOTO_BOLD_PATH = '../Report builder/Fonts/Roboto/static/Roboto-Bold.ttf'\n",
    "OUTPUT_PDF_PATH = \"../Report builder/Reports/report_from_notebook.pdf\"\n",
    "UBUNTU_REGULAR_PATH = '../Report builder/Fonts/Ubuntu/Ubuntu-Regular.ttf'\n",
    "UBUNTU_BOLD_PATH = '../Report builder/Fonts/Ubuntu/Ubuntu-Bold.ttf'\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"global-vi-backend-production.cpmugytqnb6i.eu-central-1.rds.amazonaws.com\",\n",
    "    \"database\": \"production\",\n",
    "    \"user\": \"powerbi\",\n",
    "    \"password\": \"QKWNRyS3$4X!tK&M\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- Page Geometry Constants ---\n",
    "PAGE_WIDTH, PAGE_HEIGHT = letter\n",
    "MARGIN = 72\n",
    "USABLE_WIDTH = PAGE_WIDTH - (2 * MARGIN) - 12 # 612 - 144 - 12 = 456\n",
    "\n",
    "# 2. Charting Colors\n",
    "COLOR_PALETTE = ['#119d9d', '#48aeb3', '#1a686c', '#8cd7d6', '#4d6063']\n",
    "HIGHLIGHT_COLOR = '#e86361' \n",
    "\n",
    "# 3. Business Logic\n",
    "selected_company = '40003672631'\n",
    "selected_company_reg_num = '40003672631'\n",
    "\n",
    "print(f\"Configuration set. True Usable Width is: {USABLE_WIDTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93855f04",
   "metadata": {},
   "source": [
    "## We get all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "045edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for data extranction and cleaning\n",
    "def execute_sql_query(sql_query: str):\n",
    "    \"\"\"\n",
    "    Connects to a PostgreSQL database, executes a given SQL query,\n",
    "    and returns the result as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): The SQL query to be executed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the results of the query,\n",
    "                          or None if an error occurs.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # 1. Connect to your PostgreSQL database\n",
    "        print(\"Connecting to the PostgreSQL database...\")\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        \n",
    "        # 2. Use pandas to execute the query and load it into a DataFrame\n",
    "        print(f\"Executing query: \\\"{sql_query}\\\"\")\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        print(\"Query executed successfully and data loaded into DataFrame.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Error while connecting to or querying PostgreSQL: {error}\")\n",
    "        return None  # Return None to indicate that the function failed\n",
    "    finally:\n",
    "        # 3. Ensure the connection is closed\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "427ea74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "Executing query: \"\n",
      "WITH \"PowerQuerySteps\" AS (\n",
      "    -- Step 1: Source Query from Power Query, selecting initial data\n",
      "    SELECT\n",
      "        dj.id,\n",
      "        dj.\"createdAt\",\n",
      "        dj.\"updatedAt\",\n",
      "        dj.name,\n",
      "        dj.\"salaryType\",\n",
      "        dj.\"minSalary\",\n",
      "        dj.\"maxSalary\",\n",
      "        dj.\"salaryCurrency\",\n",
      "        dj.deadline,\n",
      "        dj.\"organizationName\",\n",
      "        dj.\"infoUrl\",\n",
      "        dj.\"latvianRegion\", -- Added the original array column as requested\n",
      "        array_to_string(dj.\"latvianRegion\", ', ') AS \"latvianRegionString\",\n",
      "\n",
      "        -- Step 2: Custom Column \"IrRīga\"\n",
      "        -- This uses a CASE statement to check if 'Rīga' is an element\n",
      "        -- in the \"latvianRegion\" array.\n",
      "        CASE\n",
      "            WHEN 'RIGA' = ANY(dj.\"latvianRegion\") THEN 'Rīga'\n",
      "            ELSE 'Ārpus Rīgas'\n",
      "        END AS \"IrRīga\",\n",
      "\n",
      "        -- Step 3: Custom Column \"AverageMonthlySalary\"\n",
      "        CASE\n",
      "            WHEN dj.\"salaryType\" = 'HOURLY_RATE_BRUTTO' AND dj.\"minSalary\" < 50 AND dj.\"maxSalary\" < 50\n",
      "            THEN (dj.\"minSalary\" + dj.\"maxSalary\") / 2 * 166\n",
      "            ELSE (dj.\"minSalary\" + dj.\"maxSalary\") / 2\n",
      "        END AS \"AverageMonthlySalary\",\n",
      "\n",
      "        -- Step 4: Custom Column \"Text Between Delimiters\"\n",
      "        REPLACE(SPLIT_PART(dj.\"infoUrl\", '/', 3), 'www.google.com', 'www.cvmarket.lv') AS \"Text Between Delimiters\"\n",
      "\n",
      "    FROM\n",
      "        \"DarbnesisJob\" dj\n",
      "    WHERE\n",
      "        -- Step 5: Initial Filters\n",
      "        dj.\"infoUrl\" IS NOT NULL AND dj.\"infoUrl\" <> ''\n",
      "        AND dj.deadline <> '2026-12-31 21:59:59.999'\n",
      ")\n",
      "-- Final selection from the CTE\n",
      "SELECT\n",
      "    * -- Selects all the columns generated in the CTE above\n",
      "FROM\n",
      "    \"PowerQuerySteps\"\n",
      "WHERE\n",
      "    -- Step 6: Final Filter on the calculated salary\n",
      "    \"AverageMonthlySalary\" <= 15000\n",
      "ORDER BY\n",
      "    -- Step 7: Sorting\n",
      "    \"deadline\" DESC;\n",
      "\n",
      "\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apmer\\AppData\\Local\\Temp\\ipykernel_19556\\770523252.py:22: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully and data loaded into DataFrame.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "#Get jobs df from database\n",
    "query_to_run = \"\"\"\n",
    "WITH \"PowerQuerySteps\" AS (\n",
    "    -- Step 1: Source Query from Power Query, selecting initial data\n",
    "    SELECT\n",
    "        dj.id,\n",
    "        dj.\"createdAt\",\n",
    "        dj.\"updatedAt\",\n",
    "        dj.name,\n",
    "        dj.\"salaryType\",\n",
    "        dj.\"minSalary\",\n",
    "        dj.\"maxSalary\",\n",
    "        dj.\"salaryCurrency\",\n",
    "        dj.deadline,\n",
    "        dj.\"organizationName\",\n",
    "        dj.\"infoUrl\",\n",
    "        dj.\"latvianRegion\", -- Added the original array column as requested\n",
    "        array_to_string(dj.\"latvianRegion\", ', ') AS \"latvianRegionString\",\n",
    "\n",
    "        -- Step 2: Custom Column \"IrRīga\"\n",
    "        -- This uses a CASE statement to check if 'Rīga' is an element\n",
    "        -- in the \"latvianRegion\" array.\n",
    "        CASE\n",
    "            WHEN 'RIGA' = ANY(dj.\"latvianRegion\") THEN 'Rīga'\n",
    "            ELSE 'Ārpus Rīgas'\n",
    "        END AS \"IrRīga\",\n",
    "\n",
    "        -- Step 3: Custom Column \"AverageMonthlySalary\"\n",
    "        CASE\n",
    "            WHEN dj.\"salaryType\" = 'HOURLY_RATE_BRUTTO' AND dj.\"minSalary\" < 50 AND dj.\"maxSalary\" < 50\n",
    "            THEN (dj.\"minSalary\" + dj.\"maxSalary\") / 2 * 166\n",
    "            ELSE (dj.\"minSalary\" + dj.\"maxSalary\") / 2\n",
    "        END AS \"AverageMonthlySalary\",\n",
    "\n",
    "        -- Step 4: Custom Column \"Text Between Delimiters\"\n",
    "        REPLACE(SPLIT_PART(dj.\"infoUrl\", '/', 3), 'www.google.com', 'www.cvmarket.lv') AS \"Text Between Delimiters\"\n",
    "\n",
    "    FROM\n",
    "        \"DarbnesisJob\" dj\n",
    "    WHERE\n",
    "        -- Step 5: Initial Filters\n",
    "        dj.\"infoUrl\" IS NOT NULL AND dj.\"infoUrl\" <> ''\n",
    "        AND dj.deadline <> '2026-12-31 21:59:59.999'\n",
    ")\n",
    "-- Final selection from the CTE\n",
    "SELECT\n",
    "    * -- Selects all the columns generated in the CTE above\n",
    "FROM\n",
    "    \"PowerQuerySteps\"\n",
    "WHERE\n",
    "    -- Step 6: Final Filter on the calculated salary\n",
    "    \"AverageMonthlySalary\" <= 15000\n",
    "ORDER BY\n",
    "    -- Step 7: Sorting\n",
    "    \"deadline\" DESC;\n",
    "\n",
    "\"\"\"\n",
    "df_darbnesis_job = execute_sql_query(query_to_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "035dfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "Executing query: \"\n",
      "SELECT\n",
      "    -- Final selected columns based on \"Removed Columns\"\n",
      "    dj.id,\n",
      "    monthly_series.\"MonthAssignment\"::date,\n",
      "\n",
      "    -- \"Added Custom\" step to get the month name.\n",
      "    -- 'TMMonth' is used to get the full month name without extra padding.\n",
      "    TRIM(TO_CHAR(monthly_series.\"MonthAssignment\", 'TMMonth')) AS \"Month\"\n",
      "\n",
      "FROM\n",
      "    -- \"Source\" step: Start with the base table\n",
      "    \"DarbnesisJob\" dj\n",
      "\n",
      "-- \"Expanded MonthAssignments\" step:\n",
      "-- This is the core of the transformation. We use a CROSS JOIN LATERAL\n",
      "-- with generate_series to create a new row for each month between the\n",
      "-- start and end dates of the original row.\n",
      "CROSS JOIN LATERAL\n",
      "    generate_series(\n",
      "        -- \"StartMonth\" logic: DATE_TRUNC('month', ...) gets the first day of the month.\n",
      "        DATE_TRUNC('month', dj.\"createdAt\")::date,\n",
      "\n",
      "        -- \"EndMonth\" logic\n",
      "        DATE_TRUNC('month', dj.\"deadline\")::date,\n",
      "\n",
      "        -- Interval: The step for the series generation.\n",
      "        '1 month'::interval\n",
      "\n",
      "    ) AS monthly_series(\"MonthAssignment\") -- Alias the generated series table and its column\n",
      "\n",
      "WHERE\n",
      "    -- \"Filtered Rows\" step\n",
      "    dj.\"infoUrl\" IS NOT NULL AND dj.\"infoUrl\" <> ''\n",
      "\n",
      "    -- It's also good practice to ensure the date range is valid.\n",
      "    -- generate_series handles this gracefully by returning zero rows if start > end,\n",
      "    -- but an explicit filter can sometimes improve clarity.\n",
      "    AND dj.\"deadline\" >= dj.\"createdAt\"\n",
      "\n",
      "ORDER BY\n",
      "    dj.id, \"MonthAssignment\"; -- Optional: ordering for predictable results\n",
      "\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apmer\\AppData\\Local\\Temp\\ipykernel_19556\\770523252.py:22: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully and data loaded into DataFrame.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "#get month assignment df for all jobs\n",
    "query_to_run = \"\"\"\n",
    "SELECT\n",
    "    -- Final selected columns based on \"Removed Columns\"\n",
    "    dj.id,\n",
    "    monthly_series.\"MonthAssignment\"::date,\n",
    "    \n",
    "    -- \"Added Custom\" step to get the month name.\n",
    "    -- 'TMMonth' is used to get the full month name without extra padding.\n",
    "    TRIM(TO_CHAR(monthly_series.\"MonthAssignment\", 'TMMonth')) AS \"Month\"\n",
    "\n",
    "FROM\n",
    "    -- \"Source\" step: Start with the base table\n",
    "    \"DarbnesisJob\" dj\n",
    "    \n",
    "-- \"Expanded MonthAssignments\" step:\n",
    "-- This is the core of the transformation. We use a CROSS JOIN LATERAL\n",
    "-- with generate_series to create a new row for each month between the\n",
    "-- start and end dates of the original row.\n",
    "CROSS JOIN LATERAL\n",
    "    generate_series(\n",
    "        -- \"StartMonth\" logic: DATE_TRUNC('month', ...) gets the first day of the month.\n",
    "        DATE_TRUNC('month', dj.\"createdAt\")::date,\n",
    "        \n",
    "        -- \"EndMonth\" logic\n",
    "        DATE_TRUNC('month', dj.\"deadline\")::date,\n",
    "        \n",
    "        -- Interval: The step for the series generation.\n",
    "        '1 month'::interval\n",
    "        \n",
    "    ) AS monthly_series(\"MonthAssignment\") -- Alias the generated series table and its column\n",
    "\n",
    "WHERE\n",
    "    -- \"Filtered Rows\" step\n",
    "    dj.\"infoUrl\" IS NOT NULL AND dj.\"infoUrl\" <> ''\n",
    "    \n",
    "    -- It's also good practice to ensure the date range is valid.\n",
    "    -- generate_series handles this gracefully by returning zero rows if start > end,\n",
    "    -- but an explicit filter can sometimes improve clarity.\n",
    "    AND dj.\"deadline\" >= dj.\"createdAt\"\n",
    "\n",
    "ORDER BY\n",
    "    dj.id, \"MonthAssignment\"; -- Optional: ordering for predictable results\n",
    "\"\"\"\n",
    "df_month_assignments = execute_sql_query(query_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bdbe3b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'C:\\Users\\apmer\\Downloads\\big excelis (1)_peters (3).xlsx' sheet 'HUIJA'...\n",
      "Removed columns: ['isco-08 code', 'Mātes kompānija']\n",
      "\n",
      "Processing complete. 'df_bonusi' DataFrame created.\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19360 entries, 0 to 19359\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   id         19305 non-null  object \n",
      " 1   Attribute  19360 non-null  object \n",
      " 2   Value      19360 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 453.9+ KB\n",
      "\n",
      "--- DataFrame Head ---\n",
      "                                     id               Attribute  Value\n",
      "0  abbb48df-851f-4843-9ddb-2d3b6906a6cf  apmaksats_atvalinajums    0.0\n",
      "1  3587100c-f83c-4060-9025-9d84c8de29b1  apmaksats_atvalinajums    0.0\n",
      "2  223b9f88-dc8f-46c5-8e2f-617cf35b24bb  apmaksats_atvalinajums    0.0\n",
      "3  052b98d0-6b4f-4e17-a4b4-51616f0d55a1  apmaksats_atvalinajums    0.0\n",
      "4  c8712165-f648-4e1c-ae8e-8d92a85ff4c7  apmaksats_atvalinajums    0.0\n",
      "Reading data from 'C:\\IT Projekti\\Darbnesis Data project\\Alias LIST SSOT.xlsx'...\n",
      "\n",
      "Processing complete. 'df_alias_list' DataFrame has been created with correct types.\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17200 entries, 0 to 17199\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   organizationName     17200 non-null  object\n",
      " 1   official_name        8362 non-null   object\n",
      " 2   registration_number  8362 non-null   object\n",
      " 3   nace_codes           8362 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 537.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#get bonusu df \n",
    "file_path = r\"C:\\Users\\apmer\\Downloads\\big excelis (1)_peters (3).xlsx\"\n",
    "sheet_name = \"HUIJA\"\n",
    "\n",
    "try:\n",
    "    # Step 1, 2, 3, 4: Source, Navigation, Headers, and Type Change for 'id'\n",
    "    # pd.read_excel handles these steps. We specify the sheet_name and\n",
    "    # set the 'id' column type to string to match the Power Query logic.\n",
    "    print(f\"Reading data from '{file_path}' sheet '{sheet_name}'...\")\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, dtype={'id': str})\n",
    "    \n",
    "    # Define the columns to be removed, as specified in Power Query\n",
    "    cols_to_remove = [\"isco-08 code\", \"Mātes kompānija\"]\n",
    "    \n",
    "    # Step 5: Removed Columns\n",
    "    # Use .drop() to remove the specified columns. We add a check to ensure\n",
    "    # the columns exist before trying to drop them to avoid errors.\n",
    "    existing_cols_to_remove = [col for col in cols_to_remove if col in df.columns]\n",
    "    df_after_remove = df.drop(columns=existing_cols_to_remove)\n",
    "    print(f\"Removed columns: {existing_cols_to_remove}\")\n",
    "\n",
    "    # Step 6: Unpivoted Other Columns\n",
    "    # This is the key step. We use pandas' melt() function.\n",
    "    # - id_vars=['id']: This is the \"anchor\" column that will not be unpivoted.\n",
    "    # - var_name='Attribute': This is the name for the new column that will hold the names of the old columns.\n",
    "    # - value_name='Value': This is the name for the new column that will hold the values from the old columns.\n",
    "    df_bonusi = pd.melt(\n",
    "        df_after_remove, \n",
    "        id_vars=['id'], \n",
    "        var_name='Attribute', \n",
    "        value_name='Value'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nProcessing complete. 'df_bonusi' DataFrame created.\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_bonusi.info()\n",
    "\n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    print(df_bonusi.head())\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "#get alias list\n",
    "# Define the file path to your Excel file.\n",
    "file_path = r\"C:\\IT Projekti\\Darbnesis Data project\\Alias LIST SSOT.xlsx\"\n",
    "\n",
    "try:\n",
    "    print(f\"Reading data from '{file_path}'...\")\n",
    "    \n",
    "    # [THE FIX] Explicitly define the data type for registration_number as a string.\n",
    "    df_alias_list = pd.read_excel(file_path, dtype={'registration_number': str})\n",
    "    \n",
    "    print(\"\\nProcessing complete. 'df_alias_list' DataFrame has been created with correct types.\")\n",
    "    \n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_alias_list.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de09cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data corrected for registration number: 10103586000\n",
      "You must re-run all subsequent cells that create DataFrames from df_alias_list.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# [CORRECTED] Data Cleaning Step for df_alias_list\n",
    "# ==============================================================================\n",
    "\n",
    "# [THE FIX] Define the registration numbers as STRINGS to match the DataFrame.\n",
    "incorrect_reg_num = '10103586000'\n",
    "correct_reg_num = '50003704731'\n",
    "\n",
    "# Find a valid row with the correct registration number to use as a template.\n",
    "# This lookup will now succeed because the types match.\n",
    "correct_row_template = df_alias_list[df_alias_list['registration_number'] == correct_reg_num].iloc[0]\n",
    "correct_official_name = correct_row_template['official_name']\n",
    "correct_nace_codes = correct_row_template['nace_codes']\n",
    "\n",
    "# Apply the corrections to all rows that have the incorrect registration number.\n",
    "# We will make them identical to the correct \"POLDMA\" entry.\n",
    "df_alias_list.loc[df_alias_list['registration_number'] == incorrect_reg_num, 'registration_number'] = correct_reg_num\n",
    "df_alias_list.loc[df_alias_list['registration_number'] == incorrect_reg_num, 'official_name'] = correct_official_name\n",
    "df_alias_list.loc[df_alias_list['registration_number'] == incorrect_reg_num, 'nace_codes'] = correct_nace_codes\n",
    "\n",
    "print(f\"Data corrected for registration number: {incorrect_reg_num}\")\n",
    "print(\"You must re-run all subsequent cells that create DataFrames from df_alias_list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "115f7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'df_nace_codes' DataFrame created successfully ---\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10558 entries, 0 to 10557\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   registration_number  10558 non-null  object\n",
      " 1   nace_codes           10558 non-null  object\n",
      " 2   category             10558 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 247.6+ KB\n",
      "\n",
      "--- Final DataFrame ---\n",
      "      registration_number nace_codes category\n",
      "0             41503050998      46.83       46\n",
      "1             41503050998      46.73       46\n",
      "2             40003837873      46.71       46\n",
      "3             40003837873      23.61       23\n",
      "4             42103081417      64.20       64\n",
      "...                   ...        ...      ...\n",
      "10553         40103761688      43.31       43\n",
      "10554         40203368373      41.20       41\n",
      "10555         40003995590      25.61       25\n",
      "10556         40103394007      49.41       49\n",
      "10557         40103394007      45.32       45\n",
      "\n",
      "[10558 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#get NACE code df\n",
    "# Step 1: Select the required columns and create a working copy.\n",
    "# Also, drop any rows where 'nace_codes' is missing (None/NaN) to prevent errors.\n",
    "df_nace_codes = df_alias_list[['registration_number', 'nace_codes']].dropna().copy()\n",
    "\n",
    "# Step 2: Convert the string representation of a list into an actual Python list.\n",
    "# We use ast.literal_eval() because it's a secure way to parse Python literals.\n",
    "# Using a simple function with a try-except block makes it robust against malformed strings.\n",
    "def safe_literal_eval(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return an empty list if the string is not a valid list format\n",
    "        return []\n",
    "\n",
    "df_nace_codes['nace_codes'] = df_nace_codes['nace_codes'].apply(safe_literal_eval)\n",
    "\n",
    "# Step 3: \"Unpivot\" or \"Explode\" the list of NACE codes into separate rows.\n",
    "# The .explode() method duplicates the 'registration_number' for each item in the list.\n",
    "df_nace_codes = df_nace_codes.explode('nace_codes')\n",
    "\n",
    "# Step 4: Drop duplicate rows where the combination of registration_number and nace_codes is the same.\n",
    "df_nace_codes.drop_duplicates(subset=['registration_number', 'nace_codes'], inplace=True)\n",
    "\n",
    "# Step 5: Add the 'category' column by taking the first 2 digits of the NACE code.\n",
    "# The .str accessor lets us use string methods like slicing on the entire column.\n",
    "df_nace_codes['category'] = df_nace_codes['nace_codes'].str[:2]\n",
    "\n",
    "# Step 6: Reset the index for a clean, final DataFrame.\n",
    "df_nace_codes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n--- 'df_nace_codes' DataFrame created successfully ---\")\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df_nace_codes.info()\n",
    "\n",
    "print(\"\\n--- Final DataFrame ---\")\n",
    "print(df_nace_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "50579756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'C:\\Users\\apmer\\Downloads\\Peteris_algas+ID2 (3).xlsx' sheet 'Sheet2'...\n",
      "\n",
      "Processing complete.\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 342 entries, 0 to 349\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          342 non-null    object \n",
      " 1   new_salary  342 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 8.0+ KB\n",
      "\n",
      "--- DataFrame Head ---\n",
      "                                     id  new_salary\n",
      "0  79060d8d-d6ea-478d-ab12-687c5e4ec50d      1200.0\n",
      "1  e12026ce-77ca-4ff2-a998-faeeea322fbe      1200.0\n",
      "2  adcaba82-19e5-4b22-98b5-168788d2d230      1200.0\n",
      "3  940be8ac-2dcf-4ae8-8cda-2b36f543b8d5      1200.0\n",
      "4  ff83ceb3-476f-4cd2-a50c-5d990ff63763      1200.0\n"
     ]
    }
   ],
   "source": [
    "#get cleaned data for salaries and merge into darbnesis jobs\n",
    "file_path = r\"C:\\Users\\apmer\\Downloads\\Peteris_algas+ID2 (3).xlsx\"\n",
    "sheet_name = \"Sheet2\"\n",
    "\n",
    "try:\n",
    "    # Step 1 & 2 & 3 & 4: Source, Navigation, Promoted Headers, and initial Changed Type\n",
    "    # pandas' read_excel can handle these steps in one function call.\n",
    "    # - \"sheet_name\" targets the correct sheet.\n",
    "    # - The first row is automatically used as headers (Promoted Headers).\n",
    "    # - \"dtype={'id': str}\" sets the 'id' column to a string type, just like in Power Query.\n",
    "    print(f\"Reading data from '{file_path}' sheet '{sheet_name}'...\")\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, dtype={'id': str})\n",
    "\n",
    "    # Rename column to be more Python-friendly if desired (optional)\n",
    "    # df = df.rename(columns={\"Vidēja x slodze\": \"Videja_x_slodze\"})\n",
    "    # original_col_name = \"Videja_x_slodze\"\n",
    "    original_col_name = \"Vidēja x slodze\" # Using original name\n",
    "\n",
    "    # Step 5 & 6: Removed Errors and Changed Type for \"Vidēja x slodze\"\n",
    "    # This is the most direct way to replicate Power Query's \"Remove Errors\" logic.\n",
    "    # 1. pd.to_numeric attempts to convert the column to a number.\n",
    "    # 2. \"errors='coerce'\" will replace any value that cannot be converted with 'NaN' (Not a Number).\n",
    "    df[original_col_name] = pd.to_numeric(df[original_col_name], errors='coerce')\n",
    "\n",
    "    # 3. dropna() removes rows where the specified column ('Vidēja x slodze') is NaN.\n",
    "    #    This is the equivalent of \"Table.RemoveRowsWithErrors\".\n",
    "    df_cleaned_salary = df.dropna(subset=[original_col_name])\n",
    "    df_cleaned_salary = df_cleaned_salary.rename(columns={\"Vidēja x slodze\": \"new_salary\"})\n",
    "\n",
    "    # The 'Vidēja x slodze' column is now of a numeric type (float64 or int64).\n",
    "    # You can explicitly cast it if needed, but pd.to_numeric already handles it.\n",
    "    # df_cleaned_salary[original_col_name] = df_cleaned_salary[original_col_name].astype(float)\n",
    "    \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_cleaned_salary.info()\n",
    "\n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    print(df_cleaned_salary.head())\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "merged_df = pd.merge(df_darbnesis_job,df_cleaned_salary, on='id', how='left')\n",
    "merged_df['AverageMonthlySalary'] = np.where(\n",
    "    merged_df['new_salary'].notna(),    # Condition\n",
    "    merged_df['new_salary'],            # Value if True\n",
    "    merged_df['AverageMonthlySalary']   # Value if False\n",
    ")\n",
    "df_darbnesis_job = merged_df.drop(columns=['new_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1e3ec9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'id' and 'Isco kods' columns from 'Aprangas konkurentu vakances' in 'C:\\Users\\apmer\\Downloads\\Apranga konkurenti-īstie ISCO kodi (1).xlsx'...\n",
      "\n",
      "Processing complete. 'df_istie_isco_kodi' DataFrame created.\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         350 non-null    object\n",
      " 1   Isco kods  350 non-null    Int64 \n",
      "dtypes: Int64(1), object(1)\n",
      "memory usage: 5.9+ KB\n",
      "\n",
      "--- DataFrame Head ---\n",
      "                                     id  Isco kods\n",
      "0  79060d8d-d6ea-478d-ab12-687c5e4ec50d       5223\n",
      "1  e12026ce-77ca-4ff2-a998-faeeea322fbe       5223\n",
      "2  adcaba82-19e5-4b22-98b5-168788d2d230       5223\n",
      "3  940be8ac-2dcf-4ae8-8cda-2b36f543b8d5       5223\n",
      "4  ff83ceb3-476f-4cd2-a50c-5d990ff63763       5223\n"
     ]
    }
   ],
   "source": [
    "#get isco codes added to darbnesis job\n",
    "# Define the file path and the specific Excel Table name from your script\n",
    "file_path = r\"C:\\Users\\apmer\\Downloads\\Apranga konkurenti-īstie ISCO kodi (1).xlsx\"\n",
    "table_name = \"Aprangas konkurentu vakances\"\n",
    "\n",
    "try:\n",
    "    # This combines all the Power Query steps into one efficient operation.\n",
    "    #\n",
    "    # - sheet_name=table_name: pandas' read_excel can read from a named Excel Table\n",
    "    #   if you pass the table name as the sheet_name.\n",
    "    # - usecols=[\"id\", \"Isco kods\"]: This is the direct, efficient equivalent of\n",
    "    #   \"Table.SelectColumns\". It only reads these two columns into memory.\n",
    "    # - dtype={'id': str, 'Isco kods': 'Int64'}: This replicates \"Changed Type\".\n",
    "    #   'id' is set to string. 'Int64' (capital 'I') is pandas' nullable integer type,\n",
    "    #   which handles potential blank cells in the numeric column gracefully,\n",
    "    #   just like Power Query.\n",
    "    \n",
    "    print(f\"Reading 'id' and 'Isco kods' columns from '{table_name}' in '{file_path}'...\")\n",
    "    \n",
    "    df_istie_isco_kodi = pd.read_excel(\n",
    "        file_path,\n",
    "        sheet_name=table_name,\n",
    "        usecols=[\"id\", \"Isco kods\"],\n",
    "        dtype={'id': str, 'Isco kods': 'Int64'}\n",
    "    )\n",
    "    \n",
    "    print(\"\\nProcessing complete. 'df_istie_isco_kodi' DataFrame created.\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_istie_isco_kodi.info()\n",
    "\n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    print(df_istie_isco_kodi.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {file_path}\")\n",
    "except ValueError as e:\n",
    "    # This error often happens if the sheet/table name is not found\n",
    "    print(f\"Error reading the Excel file. Please check if a table or sheet named '{table_name}' exists.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# The final DataFrame is 'df_istie_isco_kodi'\n",
    "\n",
    "df_darbnesis_job = pd.merge(\n",
    "    df_darbnesis_job,\n",
    "    df_istie_isco_kodi,\n",
    "    on='id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7a4a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'df_distinct_orgs' DataFrame created successfully ---\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6578 entries, 0 to 6577\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   registration_number  6577 non-null   object\n",
      " 1   official_name        6577 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 102.9+ KB\n",
      "\n",
      "--- Final DataFrame ---\n",
      "     registration_number                                      official_name\n",
      "0            41503050998        Sabiedrība ar ierobežotu atbildību \"VENTUM\"\n",
      "1            40003837873                             SIA \"Plieņa dzirnavas\"\n",
      "2            42103081417                                     AS Longo Group\n",
      "3            40900040782          Ogres novada Kultūras un tūrisma pārvalde\n",
      "4            40203506125  Sabiedrība ar ierobežotu atbildību \"KK OZOLNIEKI\"\n",
      "...                  ...                                                ...\n",
      "6573         40203213162                                 AVER Brokerage SIA\n",
      "6574         40203368373     Sabiedrība ar ierobežotu atbildību \"Kasaligan\"\n",
      "6575         40003995590                                   SIA \"IDEAL LAIN\"\n",
      "6576         40103394007           Sabiedrība ar ierobežotu atbildību \"IFT\"\n",
      "6577                 NaN                                                NaN\n",
      "\n",
      "[6578 rows x 2 columns]\n",
      "\n",
      "Total unique organizations found: 6578\n"
     ]
    }
   ],
   "source": [
    "#get distinct org df\n",
    "df_distinct_orgs = df_alias_list[['registration_number', 'official_name']] \\\n",
    "    .drop_duplicates(subset=['registration_number']) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .copy()\n",
    "\n",
    "\n",
    "print(\"\\n--- 'df_distinct_orgs' DataFrame created successfully ---\")\n",
    "\n",
    "# Display info and the final result to verify\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df_distinct_orgs.info()\n",
    "\n",
    "print(\"\\n--- Final DataFrame ---\")\n",
    "print(df_distinct_orgs)\n",
    "\n",
    "print(f\"\\nTotal unique organizations found: {len(df_distinct_orgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56c603cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prepared Lookup Table (from df_alias_list) ---\n",
      "                                organizationName registration_number\n",
      "0                                     Ventum SIA         41503050998\n",
      "1                           SIA Pliena dzirnavas         40003837873\n",
      "2                                Longo Group, AS         42103081417\n",
      "3      Ogres novada Kultūras un tūrisma pārvalde         40900040782\n",
      "4                                   Kk ozolnieki         40203506125\n",
      "...                                          ...                 ...\n",
      "17174                                SIA Elmotem                 NaN\n",
      "17176                                 SIA Rakfor                 NaN\n",
      "17185                                 Sia Strela                 NaN\n",
      "17197                 Riga Golf Innovations, SIA                 NaN\n",
      "17198                             Sportland, SIA         40003530961\n",
      "\n",
      "[10256 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#add registration number column to df darbnesis job\n",
    "lookup_df = df_alias_list[['organizationName', 'registration_number']].drop_duplicates(subset=['organizationName'])\n",
    "\n",
    "print(\"\\n--- Prepared Lookup Table (from df_alias_list) ---\")\n",
    "print(lookup_df)\n",
    "\n",
    "# --- Step 2: Perform the left merge and overwrite df_darbnesis_job ---\n",
    "# We merge df_darbnesis_job with our clean lookup_df.\n",
    "# - on='organizationName': This is the column to match on.\n",
    "# - how='left': This keeps all rows from df_darbnesis_job and adds data from lookup_df where matches are found.\n",
    "df_darbnesis_job = pd.merge(\n",
    "    df_darbnesis_job,\n",
    "    lookup_df,\n",
    "    on='organizationName',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7c998e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading competitor IDs from 'C:\\IT Projekti\\Darbnesis Data project\\Aprangas konkurentu vakances.csv'...\n",
      "\n",
      "Filtered df_darbnesis_job to keep only competitor vacancies.\n",
      "Found 11 unique competitor registration numbers.\n",
      "\n",
      "--- 'df_kompanijas_konkurenti' DataFrame created successfully ---\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 2 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   Registration number             11 non-null     object\n",
      " 1   Competitor registration number  11 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 308.0+ bytes\n",
      "\n",
      "--- Final DataFrame ---\n",
      "   Registration number Competitor registration number\n",
      "0          40003672631                    40003604688\n",
      "1          40003672631                    40003530961\n",
      "2          40003672631                    40003398680\n",
      "3          40003672631                    40003813061\n",
      "4          40003672631                    40203397809\n",
      "5          40003672631                    40003548037\n",
      "6          40003672631                    40003782151\n",
      "7          40003672631                    50203193421\n",
      "8          40003672631                    50003704731\n",
      "9          40003672631                    91438477955\n",
      "10         40003672631                    11687767000\n"
     ]
    }
   ],
   "source": [
    "#add konkurentu df\n",
    "csv_file_path = r\"C:\\IT Projekti\\Darbnesis Data project\\Aprangas konkurentu vakances.csv\"\n",
    "\n",
    "# The constant registration number for your company\n",
    "aprangas_reg_number = '40003672631'\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Get the list of competitor IDs from the CSV file ---\n",
    "    # We only need the 'id' column to act as our filter.\n",
    "    # Reading it as a string ('str') ensures it will match the 'id' type in df_darbnesis_job.\n",
    "    print(f\"Reading competitor IDs from '{csv_file_path}'...\")\n",
    "    df_konkurentu_ids = pd.read_csv(csv_file_path, usecols=['id'], dtype={'id': str}, delimiter=';')\n",
    "\n",
    "    # --- Step 2: Filter df_darbnesis_job to keep only the competitor rows ---\n",
    "    # An 'inner' merge is the most direct way to do this. It keeps only the rows\n",
    "    # where the 'id' exists in both df_darbnesis_job and df_konkurentu_ids.\n",
    "    df_filtered_competitors = pd.merge(\n",
    "        df_darbnesis_job,\n",
    "        df_konkurentu_ids,\n",
    "        on='id',\n",
    "        how='inner'\n",
    "    )\n",
    "    print(\"\\nFiltered df_darbnesis_job to keep only competitor vacancies.\")\n",
    "\n",
    "    # --- Step 3: Get the distinct list of competitor registration numbers ---\n",
    "    # From the filtered DataFrame, we select the 'registration_number' column,\n",
    "    # drop any missing values (NaN), and then get the unique values.\n",
    "    competitor_reg_numbers = df_filtered_competitors['registration_number'].dropna().unique()\n",
    "    print(f\"Found {len(competitor_reg_numbers)} unique competitor registration numbers.\")\n",
    "\n",
    "    # --- Step 4: Construct the final DataFrame ---\n",
    "    # Create a new DataFrame with the specified structure.\n",
    "    # Pandas will automatically \"broadcast\" the single 'aprangas_reg_number' value\n",
    "    # to all rows, matching the length of the 'competitor_reg_numbers' list.\n",
    "    df_kompanijas_konkurenti = pd.DataFrame({\n",
    "        'Registration number': aprangas_reg_number,\n",
    "        'Competitor registration number': competitor_reg_numbers\n",
    "    })\n",
    "\n",
    "    print(\"\\n--- 'df_kompanijas_konkurenti' DataFrame created successfully ---\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_kompanijas_konkurenti.info()\n",
    "    \n",
    "    print(\"\\n--- Final DataFrame ---\")\n",
    "    print(df_kompanijas_konkurenti)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at '{csv_file_path}'. Please check the path.\")\n",
    "except KeyError:\n",
    "    print(f\"Error: The CSV file at '{csv_file_path}' does not contain an 'id' column.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5a564",
   "metadata": {},
   "source": [
    "## we define chart as class object and make helper functions for data manipulation we define some theme style guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "047d44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define helpter functions for highlighting and wrapping\n",
    "def prepare_data_with_highlight(df, full_color_palette, selected_company, highlight_color):\n",
    "    df_copy = df.copy()\n",
    "    other_colors = [color for color in full_color_palette if color != highlight_color]\n",
    "    random.shuffle(other_colors)\n",
    "    color_assignment = {}\n",
    "    other_color_index = 0\n",
    "    for company in df_copy.index:\n",
    "        if company == selected_company:\n",
    "            color_assignment[company] = highlight_color\n",
    "        else:\n",
    "            color_assignment[company] = other_colors[other_color_index % len(other_colors)]\n",
    "            other_color_index += 1\n",
    "    df_copy['barcolor'] = df_copy.index.map(color_assignment)\n",
    "    return df_copy\n",
    "\n",
    "def wrap_df_index_labels(\n",
    "    df: pd.DataFrame,\n",
    "    usable_chart_width: int = 500,\n",
    "    approx_char_pixel_width: int = 7,\n",
    "    padding_per_bar: int = 15,\n",
    "    min_wrap_limit: int = 12\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dynamically wraps the index labels of a DataFrame to fit in a chart.\n",
    "\n",
    "    This function calculates the optimal character limit for wrapping text labels\n",
    "    based on the number of data points and the available chart width. It then\n",
    "    applies this wrapping to the DataFrame's index, replacing newlines with\n",
    "    HTML line breaks (<br>) for chart rendering (e.g., in Plotly).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame whose index labels need wrapping.\n",
    "                           The function creates a copy and does not modify the original.\n",
    "        usable_chart_width (int, optional): The approximate usable pixel width\n",
    "                                             of the chart's plotting area. Defaults to 500.\n",
    "        approx_char_pixel_width (int, optional): An estimate of the average pixel\n",
    "                                                 width of a single character in the font used.\n",
    "                                                 Defaults to 7 (a good estimate for a 14px font).\n",
    "        padding_per_bar (int, optional): The amount of pixel padding to subtract\n",
    "                                         for each bar/category in the chart. Defaults to 15.\n",
    "        min_wrap_limit (int, optional): The minimum character limit to fall back on,\n",
    "                                        preventing labels from becoming too narrow.\n",
    "                                        Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the index labels formatted and wrapped.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid modifying the original DataFrame\n",
    "    prepared_df = df.copy()\n",
    "\n",
    "    # --- 1. Calculate the dynamic character limit for wrapping ---\n",
    "    number_of_bars = len(prepared_df.index)\n",
    "    if number_of_bars == 0:\n",
    "        return prepared_df # Return early if the DataFrame is empty\n",
    "\n",
    "    # Calculate available space for each label\n",
    "    space_per_bar = (usable_chart_width / number_of_bars) - padding_per_bar\n",
    "\n",
    "    # Calculate character limit based on space, with a minimum fallback\n",
    "    dynamic_char_limit = max(\n",
    "        min_wrap_limit, int(space_per_bar / approx_char_pixel_width)\n",
    "    )\n",
    "    print(f\"Dynamically calculated character limit for wrapping: {dynamic_char_limit}\")\n",
    "\n",
    "    # --- 2. Apply wrapping to the DataFrame index ---\n",
    "    formatted_labels = [\n",
    "        textwrap.fill(label, width=dynamic_char_limit).replace('\\n', '<br>')\n",
    "        for label in prepared_df.index\n",
    "    ]\n",
    "    prepared_df.index = formatted_labels\n",
    "\n",
    "    return prepared_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "25583a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# [UPGRADED] Reusable function to prepare trend data\n",
    "# ==============================================================================\n",
    "def prepare_trend_data(\n",
    "    base_jobs_df,\n",
    "    monthly_assignments_df,\n",
    "    grouping_cols,\n",
    "    aggregation_mode='count',\n",
    "    aggregation_col=None,\n",
    "    y_col_name='Value',\n",
    "    category_col_name=None,\n",
    "    nace_category=None, # Optional: for broad filtering\n",
    "    specific_nace_codes=None # Optional: for specific filtering\n",
    "):\n",
    "    \"\"\"\n",
    "    [UPGRADED] Prepares trend data. Can filter by broad NACE category OR a\n",
    "    specific list of NACE codes.\n",
    "    \"\"\"\n",
    "    print(f\"--- Preparing data for aggregation: mode='{aggregation_mode}', group_by={grouping_cols} ---\")\n",
    "    \n",
    "    # --- Step 1: Dynamic NACE Filtering ---\n",
    "    if specific_nace_codes:\n",
    "        print(f\"Filtering on specific NACE codes: {specific_nace_codes}\")\n",
    "        # Find all registration numbers that have AT LEAST ONE of the specified codes\n",
    "        sector_reg_numbers = df_nace_codes[df_nace_codes['nace_codes'].isin(specific_nace_codes)]['registration_number'].unique()\n",
    "    elif nace_category:\n",
    "        print(f\"Filtering on NACE category: {nace_category}\")\n",
    "        sector_reg_numbers = df_nace_codes[df_nace_codes['category'] == nace_category]['registration_number'].unique()\n",
    "    else:\n",
    "        print(\"Error: Must provide either 'nace_category' or 'specific_nace_codes'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- Step 2: Filter base jobs and Merge ---\n",
    "    cols_to_select = ['id'] + [col for col in grouping_cols if col in base_jobs_df.columns]\n",
    "    if aggregation_col and aggregation_col not in cols_to_select:\n",
    "        cols_to_select.append(aggregation_col)\n",
    "    sector_jobs_df = base_jobs_df[base_jobs_df['registration_number'].isin(sector_reg_numbers)][cols_to_select].copy()\n",
    "    \n",
    "    cleaned_assignments = monthly_assignments_df.dropna(subset=['Month', 'id', 'MonthAssignment'])\n",
    "    sector_jobs_with_months = pd.merge(sector_jobs_df, cleaned_assignments, on='id', how='left')\n",
    "\n",
    "    # --- Step 3: Date Filtering ---\n",
    "    sector_jobs_with_months['MonthAssignment'] = pd.to_datetime(sector_jobs_with_months['MonthAssignment'])\n",
    "    today = pd.to_datetime('today')\n",
    "    end_date = pd.Timestamp(today.year, today.month, 1)\n",
    "    start_date = end_date - pd.DateOffset(months=3)\n",
    "    last_3_months_data = sector_jobs_with_months[\n",
    "        (sector_jobs_with_months['MonthAssignment'] >= start_date) &\n",
    "        (sector_jobs_with_months['MonthAssignment'] < end_date)\n",
    "    ]\n",
    "\n",
    "    # --- Step 4: Aggregation ---\n",
    "    if aggregation_mode == 'count':\n",
    "        aggregated_data = last_3_months_data.groupby(grouping_cols).size().reset_index(name=y_col_name)\n",
    "    elif aggregation_mode == 'mean' and aggregation_col:\n",
    "        aggregated_data = last_3_months_data.groupby(grouping_cols)[aggregation_col].mean().reset_index(name=y_col_name)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- Step 5: Final Data Prep (Translate & Sort) ---\n",
    "    aggregated_data = aggregated_data.rename(columns={'Month': 'Mēnesis'})\n",
    "    if category_col_name and len(grouping_cols) > 1:\n",
    "        aggregated_data = aggregated_data.rename(columns={grouping_cols[1]: category_col_name})\n",
    "    \n",
    "    month_translation = {\n",
    "        'January': 'Janvāris', 'February': 'Februāris', 'March': 'Marts',\n",
    "        'April': 'Aprīlis', 'May': 'Maijs', 'June': 'Jūnijs',\n",
    "        'July': 'Jūlijs', 'August': 'Augusts', 'September': 'Septembris',\n",
    "        'October': 'Oktobris', 'November': 'Novembris', 'December': 'Decembris'\n",
    "    }\n",
    "    aggregated_data['Mēnesis'] = aggregated_data['Mēnesis'].map(month_translation)\n",
    "    \n",
    "    latvian_month_order = list(month_translation.values())\n",
    "    aggregated_data['Mēnesis'] = pd.Categorical(aggregated_data['Mēnesis'], categories=latvian_month_order, ordered=True)\n",
    "    aggregated_data = aggregated_data.dropna(subset=['Mēnesis']).sort_values('Mēnesis')\n",
    "    \n",
    "    print(\"Data preparation complete.\\n\")\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f8cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL, ROBUST ReportBlock CLASS (with Themed Pie Chart)\n",
    "# ==============================================================================\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class ReportBlock:\n",
    "    \"\"\"\n",
    "    [FINAL & ROBUST] The constructor is flexible. The pie chart method\n",
    "    has been corrected to use the report's custom color palette.\n",
    "    \"\"\"\n",
    "    def __init__(self, usable_width, block_title=None, block_title_style=None, description_text=None, description_style=None, layout='vertical'):\n",
    "        self.usable_width = usable_width\n",
    "        self.block_title = block_title\n",
    "        self.block_title_style = block_title_style\n",
    "        self.description_text = description_text\n",
    "        self.description_style = description_style\n",
    "        self.layout = layout\n",
    "\n",
    "    # ... (all other methods remain unchanged) ...\n",
    "    def _create_bar_chart(self, width, height, axis_format='raw'):\n",
    "        y_axis_title = self.chart_data.columns[0]\n",
    "        x_axis_title = self.chart_data.index.name or ''\n",
    "        max_val = self.chart_data[y_axis_title].max()\n",
    "        label_threshold = max_val * 0.15\n",
    "        df_inside = self.chart_data[self.chart_data[y_axis_title] >= label_threshold]\n",
    "        df_outside = self.chart_data[self.chart_data[y_axis_title] < label_threshold]\n",
    "        y_axis_config = dict(\n",
    "            title_text=f\"<b>{y_axis_title}</b>\", title_font=dict(family=\"Ubuntu\", size=30),\n",
    "            tickfont=dict(family=\"Ubuntu\", size=16), gridcolor='#C0C0C0', griddash='6px,6px', gridwidth=2,\n",
    "            showgrid=True, showline=False, ticks='', rangemode='tozero',\n",
    "            range=[0, max_val * 1.15]\n",
    "        )\n",
    "        chart_layout = go.Layout(\n",
    "            font=dict(family=\"Ubuntu\", color=\"#404040\"), plot_bgcolor='white', paper_bgcolor='white',\n",
    "            showlegend=False, barmode='overlay',\n",
    "            margin=dict(l=80, r=20, t=20, b=150),\n",
    "            xaxis=dict(\n",
    "                title_text=f\"<b>{x_axis_title}</b>\", title_font=dict(family=\"Ubuntu\", size=30), title_standoff=25,\n",
    "                tickfont=dict(family=\"Ubuntu\", size=14), showgrid=False, showline=False,\n",
    "                ticks='', tickangle=0, automargin=True\n",
    "            ),\n",
    "            yaxis=y_axis_config\n",
    "        )\n",
    "        fig = go.Figure(layout=chart_layout)\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=df_inside.index, y=df_inside[y_axis_title],\n",
    "            marker=dict(color=df_inside['barcolor'], cornerradius=10),\n",
    "            texttemplate='<b>%{y}</b>', textposition='inside',\n",
    "            insidetextfont=dict(family='Ubuntu', size=24, color='white'),\n",
    "            insidetextanchor='end', constraintext='inside'\n",
    "        ))\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=df_outside.index, y=df_outside[y_axis_title],\n",
    "            marker=dict(color=df_outside['barcolor'], cornerradius=10),\n",
    "            texttemplate='<b>%{y}</b>', textposition='outside',\n",
    "            outsidetextfont=dict(family='Ubuntu', size=24, color='#404040'),\n",
    "            cliponaxis=False\n",
    "        ))\n",
    "        img_bytes = fig.to_image(format=\"png\", width=width*3, height=height*3, scale=3)\n",
    "        return Image(BytesIO(img_bytes), width=width, height=height)\n",
    "\n",
    "    # [THE FIX] This method now accepts a 'colors' argument.\n",
    "    def _create_pie_chart(self, chart_data, colors, width, height):\n",
    "        \"\"\" Creates a pie chart. chart_data is a Pandas Series. \"\"\"\n",
    "        fig = go.Figure(data=[go.Pie(\n",
    "            labels=chart_data.index,\n",
    "            values=chart_data.values,\n",
    "            textinfo='value+percent',\n",
    "            textposition='outside',\n",
    "            hole=.3\n",
    "        )])\n",
    "        fig.update_traces(\n",
    "            textfont_size=14,\n",
    "            # [THE FIX] Applying the custom color palette to the slices.\n",
    "            marker=dict(colors=colors, line=dict(color='#FFFFFF', width=2))\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            title_text='Vakanču skaits pēc pozīcijām',\n",
    "            title_font=dict(size=16, family=\"Ubuntu\"),\n",
    "            legend_title_text='Isco-08 code',\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            margin=dict(l=20, r=20, t=50, b=20),\n",
    "            uniformtext_minsize=12, \n",
    "            uniformtext_mode='hide'\n",
    "        )\n",
    "        img_bytes = fig.to_image(format=\"png\", width=width*3, height=height*3, scale=3)\n",
    "        return Image(BytesIO(img_bytes), width=width, height=height)\n",
    "\n",
    "    def _create_line_chart(self, chart_data, x_col, y_col, category_col, selected_company, highlight_color, color_palette, x_axis_title, y_axis_title, show_legend, width, height):\n",
    "        fig = go.Figure()\n",
    "        other_colors = [color for color in color_palette if color != highlight_color]\n",
    "        random.shuffle(other_colors)\n",
    "        color_idx = 0\n",
    "        for category_name in chart_data[category_col].unique():\n",
    "            if category_name == selected_company:\n",
    "                line_color = highlight_color\n",
    "                line_width = 16\n",
    "            else:\n",
    "                line_color = other_colors[color_idx % len(other_colors)]\n",
    "                line_width = 12\n",
    "                color_idx += 1\n",
    "            category_df = chart_data[chart_data[category_col] == category_name]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=category_df[x_col], y=category_df[y_col], name=category_name,\n",
    "                mode='lines+markers', line=dict(color=line_color, width=line_width),\n",
    "                marker=dict(size=line_width)\n",
    "            ))\n",
    "        fig.update_layout(\n",
    "            font=dict(family=\"Ubuntu\", color=\"#404040\"), plot_bgcolor='white', paper_bgcolor='white',\n",
    "            showlegend=show_legend,\n",
    "            legend=dict(x=0.02, y=0.98, bgcolor='rgba(255,255,255,0.6)', font=dict(size=24, family=\"Ubuntu\")),\n",
    "            margin=dict(l=80, r=20, t=20, b=90),\n",
    "            xaxis=dict(title_text=f\"<b>{x_axis_title}</b>\", title_font=dict(size=30, family=\"Ubuntu\"), tickfont=dict(size=24, family=\"Ubuntu\"), showgrid=False),\n",
    "            yaxis=dict(title_text=f\"<b>{y_axis_title}</b>\", title_font=dict(size=30, family=\"Ubuntu\"), tickfont=dict(size=24, family=\"Ubuntu\"), gridcolor='#C0C0C0', griddash='6px,6px', gridwidth=2, rangemode='tozero')\n",
    "        )\n",
    "        img_bytes = fig.to_image(format=\"png\", width=width*3, height=height*3, scale=3)\n",
    "        return Image(BytesIO(img_bytes), width=width, height=height)\n",
    "\n",
    "    def _create_matrix_table(self, chart_data, index_col, columns_col, values_col):\n",
    "        matrix_df = chart_data.pivot_table(index=index_col, columns=columns_col, values=values_col, aggfunc='nunique').fillna(0).astype(int)\n",
    "        total_uniques = chart_data.groupby(index_col)[values_col].nunique()\n",
    "        total_uniques.name = 'Total'\n",
    "        final_matrix_df = matrix_df.join(total_uniques)\n",
    "        month_order = ['April', 'May', 'June']\n",
    "        present_months = [col for col in final_matrix_df.columns if col in month_order]\n",
    "        sorted_months = sorted(present_months, key=lambda m: month_order.index(m))\n",
    "        final_matrix_df = final_matrix_df[sorted_months + ['Total']]\n",
    "        final_matrix_df = final_matrix_df.sort_values(by='Total', ascending=False)\n",
    "        total_row = final_matrix_df.sum().to_frame().T\n",
    "        total_row.index = ['Total']\n",
    "        final_matrix_df = pd.concat([final_matrix_df, total_row])\n",
    "        header = ['Mātes kompānija'] + [str(col) for col in final_matrix_df.columns]\n",
    "        data_for_table = [header] + [[index] + row.tolist() for index, row in final_matrix_df.iterrows()]\n",
    "        table = Table(data_for_table, hAlign='LEFT')\n",
    "        table_style = TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Ubuntu'), ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'), ('GRID', (0, 0), (-1, -1), 0.5, colors.lightgrey),\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Ubuntu-Bold'), ('LINEBELOW', (0, 0), (-1, 0), 1.5, colors.HexColor(\"#05a1b4\")),\n",
    "            ('ALIGN', (1, 0), (-1, 0), 'CENTER'), ('FONTNAME', (0, -1), (-1, -1), 'Ubuntu-Bold'),\n",
    "            ('BACKGROUND', (0, -1), (-1, -1), colors.whitesmoke), ('LINEABOVE', (0, -1), (-1, -1), 1.5, colors.darkgrey),\n",
    "            ('FONTNAME', (-1, 1), (-1, -1), 'Ubuntu-Bold'), ('ALIGN', (0, 0), (0, -1), 'LEFT'),\n",
    "            ('RIGHTPADDING', (0, 0), (0, -1), 12), ('ALIGN', (1, 1), (-1, -1), 'CENTER'),\n",
    "        ])\n",
    "        for i in range(1, len(final_matrix_df)):\n",
    "            if i % 2 == 0:\n",
    "                table_style.add('BACKGROUND', (0, i), (-1, i), colors.HexColor(\"#f0f0f0\"))\n",
    "        table.setStyle(table_style)\n",
    "        return table\n",
    "\n",
    "    def _create_ribbon_chart(self, chart_data, colors, width, height):\n",
    "        fig = go.Figure()\n",
    "        for company in chart_data.index:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=chart_data.columns, y=chart_data.loc[company], name=company,\n",
    "                mode='lines', line=dict(width=0.5, color='white'),\n",
    "                stackgroup='one', fillcolor=colors.get(company, '#cccccc')\n",
    "            ))\n",
    "        fig.update_layout(\n",
    "            title_text='Vakanču skaits', title_font=dict(size=16, family=\"Ubuntu\"),\n",
    "            showlegend=True, legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "            plot_bgcolor='white', paper_bgcolor='white', xaxis=dict(showgrid=False),\n",
    "            yaxis=dict(title_text='Count of id', type='linear', showgrid=True, gridcolor='#e0e0e0'),\n",
    "            margin=dict(l=50, r=20, t=50, b=20)\n",
    "        )\n",
    "        img_bytes = fig.to_image(format=\"png\", width=width*3, height=height*3, scale=3)\n",
    "        return Image(BytesIO(img_bytes), width=width, height=height)\n",
    "    \n",
    "    def render(self, chart_type, **kwargs):\n",
    "        elements = []\n",
    "        if self.block_title and self.block_title_style:\n",
    "            elements.append(Paragraph(self.block_title, self.block_title_style))\n",
    "            elements.append(Spacer(1, 6))\n",
    "        visual_element = None\n",
    "        if chart_type == 'bar':\n",
    "            width_percentage = kwargs.pop('width_percentage', 0.98)\n",
    "            image_width = self.usable_width * width_percentage\n",
    "            aspect_ratio = kwargs.pop('aspect_ratio', 0.40)\n",
    "            image_height = image_width * aspect_ratio\n",
    "            visual_element = self._create_bar_chart(width=image_width, height=image_height, **kwargs)\n",
    "        elif chart_type == 'line':\n",
    "            width_percentage = kwargs.pop('width_percentage', 0.98)\n",
    "            image_width = self.usable_width * width_percentage\n",
    "            aspect_ratio = kwargs.pop('aspect_ratio', 0.6)\n",
    "            image_height = image_width * aspect_ratio\n",
    "            visual_element = self._create_line_chart(width=image_width, height=image_height, **kwargs)\n",
    "        elif chart_type == 'matrix':\n",
    "            visual_element = self._create_matrix_table(**kwargs)\n",
    "        elif chart_type == 'ribbon':\n",
    "            width_percentage = kwargs.pop('width_percentage', 0.98)\n",
    "            image_width = self.usable_width * width_percentage\n",
    "            aspect_ratio = 0.6\n",
    "            image_height = image_width * aspect_ratio\n",
    "            visual_element = self._create_ribbon_chart(width=image_width, height=image_height, **kwargs)\n",
    "        elif chart_type == 'pie':\n",
    "            width_percentage = kwargs.pop('width_percentage', 0.98)\n",
    "            image_width = self.usable_width * width_percentage\n",
    "            aspect_ratio = 0.8\n",
    "            image_height = image_width * aspect_ratio\n",
    "            visual_element = self._create_pie_chart(width=image_width, height=image_height, **kwargs)\n",
    "\n",
    "        if not visual_element: return []\n",
    "        if self.layout == 'horizontal' and self.description_text:\n",
    "            text_element = Paragraph(self.description_text, self.description_style)\n",
    "            container_table = Table(\n",
    "                [[text_element, visual_element]],\n",
    "                colWidths=[self.usable_width * 0.35, self.usable_width * 0.65],\n",
    "                hAlign='LEFT'\n",
    "            )\n",
    "            container_table.setStyle(TableStyle([('VALIGN', (0,0), (-1,-1), 'TOP')]))\n",
    "            elements.append(container_table)\n",
    "        else:\n",
    "            elements.append(visual_element)\n",
    "            if self.description_text:\n",
    "                elements.append(Spacer(1, 6))\n",
    "                elements.append(Paragraph(self.description_text, self.description_style))\n",
    "        elements.append(Spacer(1, 18))\n",
    "        return [KeepTogether(elements)]\n",
    "\n",
    "    def render_bar_chart(self, chart_data, **kwargs):\n",
    "        self.chart_data = chart_data\n",
    "        return self.render('bar', **kwargs)\n",
    "    def render_line_chart(self, **kwargs):\n",
    "        return self.render('line', **kwargs)\n",
    "    def render_matrix_table(self, **kwargs):\n",
    "        return self.render('matrix', **kwargs)\n",
    "    def render_ribbon_chart(self, **kwargs):\n",
    "        return self.render('ribbon', **kwargs)\n",
    "    def render_pie_chart(self, **kwargs):\n",
    "        return self.render('pie', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "69f6d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HIGHLIGHT FIRST ---\n",
    "df_distinct_orgs = prepare_data_with_highlight(\n",
    "    df=df_distinct_orgs,\n",
    "    full_color_palette=COLOR_PALETTE,\n",
    "    selected_company=selected_company,\n",
    "    highlight_color=HIGHLIGHT_COLOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f2a7b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All fonts and font families registered successfully.\n",
      "Report styles defined. Created separate style for chart titles.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CENTRALIZED REPORT STYLING AND SETUP (with separate chart title style)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Font Registration & Family Definitions ---\n",
    "try:\n",
    "    pdfmetrics.registerFont(TTFont('Roboto', ROBOTO_REGULAR_PATH))\n",
    "    pdfmetrics.registerFont(TTFont('Roboto-Bold', ROBOTO_BOLD_PATH))\n",
    "    pdfmetrics.registerFont(TTFont('Ubuntu', UBUNTU_REGULAR_PATH))\n",
    "    pdfmetrics.registerFont(TTFont('Ubuntu-Bold', UBUNTU_BOLD_PATH))\n",
    "    pdfmetrics.registerFontFamily('Roboto', normal='Roboto', bold='Roboto-Bold')\n",
    "    pdfmetrics.registerFontFamily('Ubuntu', normal='Ubuntu', bold='Ubuntu-Bold')\n",
    "    print(\"All fonts and font families registered successfully.\")\n",
    "    main_font = 'Ubuntu'\n",
    "    main_font_bold = 'Ubuntu-Bold'\n",
    "except Exception as e:\n",
    "    print(f\"Error registering fonts, defaulting to Roboto. Error: {e}\")\n",
    "    pdfmetrics.registerFontFamily('Roboto', normal='Roboto', bold='Roboto-Bold')\n",
    "    main_font = 'Roboto'\n",
    "    main_font_bold = 'Roboto-Bold'\n",
    "\n",
    "# --- 2. Define All Paragraph Styles ---\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Style for colored main section titles (e.g., \"1. Ievads\")\n",
    "intro_title_style = ParagraphStyle(\n",
    "    'introTitle', parent=styles['h2'], fontName=main_font_bold,\n",
    "    fontSize=24, leading=28, spaceAfter=12, textColor='#05a1b4'\n",
    ")\n",
    "\n",
    "# Style for TEXT sub-section titles (e.g., \"3.1. ...\")\n",
    "subsection_title_style = ParagraphStyle(\n",
    "    'subsectionTitle', parent=styles['h3'], fontName=main_font_bold,\n",
    "    fontSize=18,      # Reverted to 18pt\n",
    "    leading=22,       # Reverted to 22pt\n",
    "    spaceAfter=6,\n",
    "    textColor='#1a686c'\n",
    ")\n",
    "\n",
    "# [NEW] Style specifically for CHART titles (e.g., \"Vakanču skaits\")\n",
    "chart_title_style = ParagraphStyle(\n",
    "    'chartTitle', parent=styles['h3'], fontName=main_font_bold,\n",
    "    fontSize=14,      # Set to 16pt as requested\n",
    "    leading=20,\n",
    "    spaceAfter=8,\n",
    "    textColor='#1a686c'\n",
    ")\n",
    "\n",
    "# Style for all standard body text\n",
    "intro_body_style = ParagraphStyle(\n",
    "    'introBody', parent=styles['BodyText'], fontName=main_font,\n",
    "    fontSize=11, leading=15, spaceAfter=12\n",
    ")\n",
    "\n",
    "# Style for bolded sub-headings within body text\n",
    "sub_heading_style = ParagraphStyle(\n",
    "    'subHeading', parent=intro_body_style, fontName=main_font_bold, spaceAfter=6\n",
    ")\n",
    "\n",
    "# Style for indented bullet points\n",
    "bullet_style = ParagraphStyle(\n",
    "    'bulletStyle', parent=intro_body_style, leftIndent=20, spaceAfter=6\n",
    ")\n",
    "\n",
    "# --- 3. Initialize the Document Story ---\n",
    "story = []\n",
    "\n",
    "print(\"Report styles defined. Created separate style for chart titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6869fb5",
   "metadata": {},
   "source": [
    "## we build the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cbb4dbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction section has been added to the story.\n"
     ]
    }
   ],
   "source": [
    "#Ievads\n",
    "\n",
    "\n",
    "title_text = \"1. Ievads\"\n",
    "story.append(Paragraph(title_text, intro_title_style))\n",
    "\n",
    "# Add the body text, transcribed from the image\n",
    "body_text_p1 = \"\"\"\n",
    "Šī atskaite sniedz uz datiem balstītu pārskatu par darba tirgus situāciju Latvijas mazumtirdzniecības\n",
    "nozarē, īpašu uzmanību pievēršot SIA “Apranga” vakanču aktivitātei. Salīdzinājumam analizētas arī\n",
    "galveno konkurentu – LPP Latvia, H&M Hennes & Mauritz SIA un Danija – publicētās vakances pēdējo\n",
    "trīs mēnešu laikā.\n",
    "\"\"\"\n",
    "story.append(Paragraph(body_text_p1, intro_body_style))\n",
    "\n",
    "body_text_p2 = \"\"\"\n",
    "Atskaitē iekļauta vakanču dinamika dažādos reģionos, aktuālākie amata veidi un prasības, kā arī\n",
    "sezonālās tendences un kandidātu meklēšanas paradumi. Detalizēti apskatīts arī atalgojuma un\n",
    "bonusu salīdzinājums starp uzņēmumiem.\n",
    "\"\"\"\n",
    "story.append(Paragraph(body_text_p2, intro_body_style))\n",
    "\n",
    "body_text_p3 = \"\"\"\n",
    "Analīze balstīta uz datiem no darba portāla darbe.lv, un to sagatavojis uzņēmums SIA “Visasiespējas”.\n",
    "\"\"\"\n",
    "story.append(Paragraph(body_text_p3, intro_body_style))\n",
    "\n",
    "body_text_p4 = \"\"\"\n",
    "<b>Mērķis</b> – sniegt SIA “Apranga” praktiskas rekomendācijas, kas palīdzētu uzlabot personāla\n",
    "piesaistes efektivitāti un konkurētspēju darba tirgū.\n",
    "\"\"\"\n",
    "story.append(Paragraph(body_text_p4, intro_body_style))\n",
    "\n",
    "# Add a larger space before the next section\n",
    "story.append(Spacer(1, 24))\n",
    "\n",
    "print(\"Introduction section has been added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f04bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary section has been added to the story.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. KOPSAVILKUMS (SUMMARY) ---\n",
    "\n",
    "# Add the main title for the section\n",
    "section_title_2 = \"2. Kopsavilkums\"\n",
    "story.append(Paragraph(section_title_2, intro_title_style))\n",
    "\n",
    "# Add the first main paragraph\n",
    "summary_p1 = \"\"\"\n",
    "Šī atskaite sniedz padziļinātu analīzi par darba tirgus situāciju Latvijas apģērbu mazumtirdzniecības\n",
    "nozarē, ar uzsvaru uz SIA “Apranga” konkurētspēju personāla piesaistes jomā. Analīze balstās uz\n",
    "2025. gada aprīļa–jūnija periodā apkopotiem datiem no darba portāla Darbe.lv un ietver galveno\n",
    "konkurentu – LPP Latvia, H&M, Danija u.c. – salīdzinājumu.\n",
    "\"\"\"\n",
    "story.append(Paragraph(summary_p1, intro_body_style))\n",
    "\n",
    "# Add the sub-heading for the key findings\n",
    "summary_sub_heading = \"Galvenie secinājumi:\"\n",
    "story.append(Paragraph(summary_sub_heading, sub_heading_style))\n",
    "\n",
    "# Add the bullet points\n",
    "bullet_points = [\n",
    "    \"SIA “Apranga” saglabā stabilu pozīciju darba tirgū ar konkurētspējīgu vakanču skaitu un atalgojumu, pārsniedzot nozares vidējo līmeni.\",\n",
    "    \"Vakances koncentrētas Rīgā, kas atbilst uzņēmuma pašreizējam veikalu izvietojumam. Taču konkurenti aktīvāk darbojas arī reģionos, piedāvājot plašākas iespējas kandidātiem ārpus galvaspilsētas.\",\n",
    "    \"Populārākā amata pozīcija visā nozarē ir pārdevējs veikalā (ISCO-08 kods 5223), kam seko veikalu vadītāji.\",\n",
    "    \"Apranga izceļas ar konsekventu elastīgā darba grafika un veselības apdrošināšanas piedāvājumu, taču sludinājumos trūkst komunikācijas par citiem populāriem bonusiem (piemēram, prēmijām, apmaksātām pusdienām, darbinieku saliedēšanas pasākumiem)...\"\n",
    "]\n",
    "\n",
    "for point in bullet_points:\n",
    "    # Prepend the bullet character to the text\n",
    "    bullet_paragraph = Paragraph(f\"• {point}\", bullet_style)\n",
    "    story.append(bullet_paragraph)\n",
    "\n",
    "# Add the final concluding paragraph\n",
    "summary_p2 = \"\"\"\n",
    "<b>Balstoties uz iegūtajiem datiem, sniegtas praktiskas rekomendācijas personāla piesaistes uzlabošanai,</b> tostarp uzlabot sludinājumu saturu ar pilnīgāku bonusu aprakstu, diferencēt komunikāciju pret studentu auditoriju, un apsvērt referāļu programmas ieviešanu.\n",
    "\"\"\"\n",
    "story.append(Paragraph(summary_p2, intro_body_style))\n",
    "\n",
    "# Add a larger space before the next section\n",
    "story.append(Spacer(1, 24))\n",
    "\n",
    "print(\"Summary section has been added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f05bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Market Overview section has been added to the story.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. VISPĀRĒJAIS TIRGUS APSKATS (GENERAL MARKET OVERVIEW) ---\n",
    "\n",
    "# Add the main title for the section\n",
    "section_title_3 = \"3. Vispārējais tirgus apskats\"\n",
    "story.append(Paragraph(section_title_3, intro_title_style))\n",
    "\n",
    "# Add the introductory paragraph for this section\n",
    "market_overview_p1 = \"\"\"\n",
    "Šī sadaļa sniedz pārskatu par mazumtirdzniecības darba tirgus attīstību Latvijā pēdējo trīs mēnešu\n",
    "laikā, izceļot būtiskākās izmaiņas un aktuālās tendences. Analīze ir strukturēta divās daļās:\n",
    "\"\"\"\n",
    "story.append(Paragraph(market_overview_p1, intro_body_style))\n",
    "\n",
    "# Add the first sub-section (3.1)\n",
    "# Using <b> and <i> tags for inline formatting\n",
    "market_overview_p2 = \"\"\"\n",
    "<b>3.1. daļa</b> aplūko mazumtirdzniecības nozari kopumā, koncentrējoties uz vakancēm, kas atbilst NACE\n",
    "klasifikācijas kodam <b>47 “Mazumtirdzniecība, izņemot automobiļus un motociklus”</b>. Šajā kategorijā\n",
    "ietilpst, piemēram, tādi uzņēmumi kā <i>Maxima, Circle K</i> un <i>Apranga</i>.\n",
    "\"\"\"\n",
    "story.append(Paragraph(market_overview_p2, intro_body_style))\n",
    "\n",
    "# Add a small spacer for visual separation before the next part\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Add the second sub-section (3.2)\n",
    "market_overview_p3 = \"\"\"\n",
    "<b>3.2. daļa</b> fokusējas uz apģērbu mazumtirdzniecību specializētajos veikalos, kas klasificējas ar NACE\n",
    "kodu <b>47.1</b>, sniedzot padziļinātu vakanču analīzi šajā apakšnozarē.\n",
    "\"\"\"\n",
    "story.append(Paragraph(market_overview_p3, intro_body_style))\n",
    "\n",
    "# Add the final concluding paragraph for the section\n",
    "market_overview_p4 = \"\"\"\n",
    "Analīzē izmantoti dati no populārākajiem darba meklēšanas portāliem, iekļaujot galvenos rādītājus\n",
    "par vakanču <b>skaitu</b>, vidējo <b>atalgojumu</b> un to <b>ģeogrāfisko sadalījumu</b>. Šis pārskats kalpo kā pamats\n",
    "konkurentu salīdzināšanai un palīdz pieņemt pamatotus, stratēģiskus lēmumus personāla vadībā.\n",
    "\"\"\"\n",
    "story.append(Paragraph(market_overview_p4, intro_body_style))\n",
    "\n",
    "\n",
    "# Add a larger space before the next section\n",
    "story.append(Spacer(1, 24))\n",
    "\n",
    "print(\"General Market Overview section has been added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "47865319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically determined general NACE category: 47\n",
      "--- Preparing data for aggregation: mode='count', group_by=['Month'] ---\n",
      "Filtering on NACE category: 47\n",
      "Data preparation complete.\n",
      "\n",
      "--- Preparing data for aggregation: mode='mean', group_by=['Month'] ---\n",
      "Filtering on NACE category: 47\n",
      "Data preparation complete.\n",
      "\n",
      "--- Preparing data for aggregation: mode='count', group_by=['Month', 'IrRīga'] ---\n",
      "Filtering on NACE category: 47\n",
      "Data preparation complete.\n",
      "\n",
      "\n",
      "Section 3.1 (NACE: 47) has been built with widow control and added to the story.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3.1: VISPĀRĒJAIS MAZUMTIRDZNIECĪBAS TIRGUS [CORRECTED WIDOW CONTROL]\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Step 1: Dynamically determine the General NACE Category for the selected company ---\n",
    "try:\n",
    "    general_nace_category = df_nace_codes[df_nace_codes['registration_number'] == selected_company_reg_num]['category'].iloc[0]\n",
    "    print(f\"Dynamically determined general NACE category: {general_nace_category}\")\n",
    "except (IndexError, KeyError) as e:\n",
    "    print(f\"Warning: Could not determine general NACE category for {selected_company_reg_num}. Falling back to '47'. Error: {e}\")\n",
    "    general_nace_category = '47'\n",
    "\n",
    "# --- Step 2: Group the title with the first chart to prevent widows ---\n",
    "# [THE FIX] Create a temporary list to hold the elements that must stay together.\n",
    "title_and_first_chart = []\n",
    "\n",
    "# Add the title to this temporary list\n",
    "title_and_first_chart.append(Paragraph(f\"3.1. Vispārējais mazumtirdzniecības tirgus (NACE kods {general_nace_category})\", subsection_title_style))\n",
    "title_and_first_chart.append(Spacer(1, 12))\n",
    "\n",
    "# --- Chart 1: Total Vacancy Count (General Retail) ---\n",
    "vacancy_count_data_gen = prepare_trend_data(\n",
    "    base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "    nace_category=general_nace_category, \n",
    "    grouping_cols=['Month'], \n",
    "    aggregation_mode='count', y_col_name='Vakanču skaits'\n",
    ")\n",
    "if not vacancy_count_data_gen.empty:\n",
    "    vacancy_count_data_gen['category'] = 'Vakanču skaits'\n",
    "    block = ReportBlock(\n",
    "        usable_width=USABLE_WIDTH, \n",
    "        block_title=\"Vakanču skaits (pēd. 3 mēn.)\", \n",
    "        block_title_style=chart_title_style\n",
    "    )\n",
    "    elements = block.render_line_chart(\n",
    "        chart_data=vacancy_count_data_gen, x_col='Mēnesis', y_col='Vakanču skaits', category_col='category',\n",
    "        selected_company='Vakanču skaits', highlight_color='#05a1b4', color_palette=COLOR_PALETTE,\n",
    "        x_axis_title='Mēnesis', y_axis_title='Kopējais vakanču skaits', show_legend=False, width_percentage=0.8,\n",
    "        aspect_ratio=0.5 \n",
    "    )\n",
    "    # Add the first chart to the temporary list\n",
    "    title_and_first_chart.extend(elements)\n",
    "\n",
    "# Add the combined group to the main story, wrapped in KeepTogether\n",
    "story.append(KeepTogether(title_and_first_chart))\n",
    "story.append(Spacer(1, 18))\n",
    "\n",
    "\n",
    "# --- Step 3: Add the remaining charts directly to the story ---\n",
    "\n",
    "# --- Chart 2: Average Salary (General Retail) ---\n",
    "avg_salary_data_gen = prepare_trend_data(\n",
    "    base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "    nace_category=general_nace_category, \n",
    "    grouping_cols=['Month'], \n",
    "    aggregation_mode='mean', aggregation_col='AverageMonthlySalary', y_col_name='Vidējā mēnešalga (€)'\n",
    ")\n",
    "if not avg_salary_data_gen.empty:\n",
    "    avg_salary_data_gen['category'] = 'Vidējā alga'\n",
    "    block = ReportBlock(\n",
    "        usable_width=USABLE_WIDTH, \n",
    "        block_title=\"Vidējā alga (pēd. 3 mēn.)\", \n",
    "        block_title_style=chart_title_style\n",
    "    )\n",
    "    elements = block.render_line_chart(\n",
    "        chart_data=avg_salary_data_gen, x_col='Mēnesis', y_col='Vidējā mēnešalga (€)', category_col='category',\n",
    "        selected_company='Vidējā alga', highlight_color='#05a1b4', color_palette=COLOR_PALETTE,\n",
    "        x_axis_title='Mēnesis', y_axis_title='Vidējā mēnešalga (€)', show_legend=False, width_percentage=0.8,\n",
    "        aspect_ratio=0.5\n",
    "    )\n",
    "    story.extend(elements)\n",
    "    story.append(Spacer(1, 18))\n",
    "\n",
    "# --- Chart 3: Vacancies by Location (General Retail) ---\n",
    "location_data_gen = prepare_trend_data(\n",
    "    base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "    nace_category=general_nace_category, \n",
    "    grouping_cols=['Month', 'IrRīga'], \n",
    "    aggregation_mode='count', y_col_name='Vakanču skaits', category_col_name='Atrašanās vieta'\n",
    ")\n",
    "if not location_data_gen.empty:\n",
    "    block = ReportBlock(\n",
    "        usable_width=USABLE_WIDTH,\n",
    "        block_title=\"Vakanču sadalījums: Rīga vs. reģioni (pēd. 3 mēn.)\",\n",
    "        block_title_style=chart_title_style\n",
    "    )\n",
    "    elements = block.render_line_chart(\n",
    "        chart_data=location_data_gen, x_col='Mēnesis', y_col='Vakanču skaits', category_col='Atrašanās vieta',\n",
    "        selected_company=None, highlight_color=HIGHLIGHT_COLOR, color_palette=COLOR_PALETTE,\n",
    "        x_axis_title='Mēnesis', y_axis_title='Kopējais vakanču skaits', show_legend=True, width_percentage=0.8,\n",
    "        aspect_ratio=0.5\n",
    "    )\n",
    "    story.extend(elements)\n",
    "\n",
    "print(f\"\\nSection 3.1 (NACE: {general_nace_category}) has been built with widow control and added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "51c32a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing data for aggregation: mode='count', group_by=['Month'] ---\n",
      "Filtering on specific NACE codes: ['47.71']\n",
      "Data preparation complete.\n",
      "\n",
      "--- Preparing data for aggregation: mode='mean', group_by=['Month'] ---\n",
      "Filtering on specific NACE codes: ['47.71']\n",
      "Data preparation complete.\n",
      "\n",
      "--- Preparing data for aggregation: mode='count', group_by=['Month', 'IrRīga'] ---\n",
      "Filtering on specific NACE codes: ['47.71']\n",
      "Data preparation complete.\n",
      "\n",
      "\n",
      "Section 3.2 has been built with flexible layout and added to the story.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3.2: APĢĒRBU MAZUMTIRDZNIECĪBA [FINAL LAYOUT FIX]\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Step 1: Add all components directly to the main story ---\n",
    "# [THE FIX] We are no longer using a temporary list or a single large KeepTogether wrapper.\n",
    "\n",
    "# Add the title and introductory paragraph\n",
    "story.append(Paragraph(\"3.2. Apģērbu mazumtirdzniecība specializētajos veikalos vakanču analīze (NACE kods 47.1)\", subsection_title_style))\n",
    "story.append(Paragraph(\n",
    "    \"\"\"\n",
    "    Šajā sadaļā apskatītas darba tirgus tendences uzņēmumos, kas darbojas apģērbu mazumtirdzniecībā\n",
    "    specializētajos veikalos. Dati aptver laika periodu no 2025. gada aprīļa līdz jūnijam un atspoguļo\n",
    "    vakanču skaita, lokācijas un vidējās algas dinamiku. Svarīgi uzsvērt, ka šāda veida dati sniedz ieskatu\n",
    "    par nozares aktivitāti un potenciālajām izmaiņām darbaspēka pieprasījumā.\n",
    "    \"\"\",\n",
    "    intro_body_style\n",
    "))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# --- Step 2: Get the specific NACE codes to use for filtering ---\n",
    "try:\n",
    "    specialized_nace_codes = df_nace_codes[df_nace_codes['registration_number'] == selected_company_reg_num]['nace_codes'].unique().tolist()\n",
    "except Exception as e:\n",
    "    specialized_nace_codes = []\n",
    "\n",
    "# --- Step 3: Generate the charts and their full descriptions ---\n",
    "if specialized_nace_codes:\n",
    "    # --- Chart 1: Total Vacancy Count (Specialized) ---\n",
    "    vacancy_count_data = prepare_trend_data(\n",
    "        base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "        specific_nace_codes=specialized_nace_codes, grouping_cols=['Month'], \n",
    "        aggregation_mode='count', y_col_name='Vakanču skaits'\n",
    "    )\n",
    "    if not vacancy_count_data.empty:\n",
    "        vacancy_count_data['category'] = 'Vakanču skaits'\n",
    "        block = ReportBlock(\n",
    "            usable_width=USABLE_WIDTH, \n",
    "            block_title=\"Vakanču skaits (pēd. 3 mēn.)\", \n",
    "            block_title_style=chart_title_style\n",
    "        )\n",
    "        elements = block.render_line_chart(\n",
    "            chart_data=vacancy_count_data, x_col='Mēnesis', y_col='Vakanču skaits', category_col='category',\n",
    "            selected_company='Vakanču skaits', highlight_color='#05a1b4', color_palette=COLOR_PALETTE,\n",
    "            x_axis_title='Mēnesis', y_axis_title='Kopējais vakanču skaits', show_legend=False, width_percentage=0.8,\n",
    "            aspect_ratio=0.5 # Using the smaller aspect ratio\n",
    "        )\n",
    "        story.extend(elements)\n",
    "        \n",
    "        story.append(Paragraph(\n",
    "            \"\"\"\n",
    "            Šis grafiks ilustrē kopējo vakanču skaitu nozarē laika gaitā. Dati <b>uzrāda lēnu, bet pakāpenisku\n",
    "            samazinājumu</b> no aprīļa līdz jūnijam. Šī tendence var liecināt par vairākām iespējamām norisēm —\n",
    "            sezonālās darbaspēka vajadzības var būt jau apmierinātas, vai arī uzņēmumi kļūst selektīvāki un\n",
    "            samazina aktīvo sludinājumu skaitu vasaras sezonā. Samazinājums var būt saistīts arī ar <b>darba tirgus\n",
    "            piesātinājumu</b> vai <b>lielāku darbinieku noturību</b> pēc pavasara pieņemšanas cikliem.\n",
    "            \"\"\",\n",
    "            intro_body_style\n",
    "        ))\n",
    "        story.append(Spacer(1, 18))\n",
    "\n",
    "    # --- Chart 2: Average Salary (Specialized) ---\n",
    "    avg_salary_data = prepare_trend_data(\n",
    "        base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "        specific_nace_codes=specialized_nace_codes, grouping_cols=['Month'], \n",
    "        aggregation_mode='mean', aggregation_col='AverageMonthlySalary', y_col_name='Vidējā mēnešalga (€)'\n",
    "    )\n",
    "    if not avg_salary_data.empty:\n",
    "        avg_salary_data['category'] = 'Vidējā alga'\n",
    "        block = ReportBlock(\n",
    "            usable_width=USABLE_WIDTH, \n",
    "            block_title=\"Vidējā alga (pēd. 3 mēn.)\", \n",
    "            block_title_style=chart_title_style\n",
    "        )\n",
    "        elements = block.render_line_chart(\n",
    "            chart_data=avg_salary_data, x_col='Mēnesis', y_col='Vidējā mēnešalga (€)', category_col='category',\n",
    "            selected_company='Vidējā alga', highlight_color='#05a1b4', color_palette=COLOR_PALETTE,\n",
    "            x_axis_title='Mēnesis', y_axis_title='Vidējā mēnešalga (€)', show_legend=False, width_percentage=0.8,\n",
    "            aspect_ratio=0.5 # Using the smaller aspect ratio\n",
    "        )\n",
    "        story.extend(elements)\n",
    "\n",
    "        story.append(Paragraph(\n",
    "            \"\"\"\n",
    "            Šajā grafikā redzams, ka vidējā mēnešalga analizētajā nozarē (NACE 47.71) ir <b>stabila,</b> bez būtiskām\n",
    "            svārstībām — <b>aptuveni 1 000 EUR mēnesī.</b> Šī stabilitāte norāda uz prognozējamu un strukturētu\n",
    "            atalgojuma politiku šajā nozarē. Tomēr, svarīgi ņemt vērā iespējamos <b>slodzes ietekmi uz norādīto\n",
    "            summu:</b> darba devēji ne vienmēr precizē, vai minētais atalgojums attiecas uz pilnu slodzi, kas var\n",
    "            radīt neobjektīvu priekšstatu par reālo atalgojumu konkrētajā pozīcijā. Tāpēc šos datus jāinterpretē\n",
    "            piesardzīgi un vēlams kontekstā ar sludinājuma saturu.\n",
    "            \"\"\",\n",
    "            intro_body_style\n",
    "        ))\n",
    "        story.append(Spacer(1, 18))\n",
    "\n",
    "    # --- Chart 3: Vacancies by Location (Specialized) ---\n",
    "    location_data = prepare_trend_data(\n",
    "        base_jobs_df=df_darbnesis_job, monthly_assignments_df=df_month_assignments,\n",
    "        specific_nace_codes=specialized_nace_codes, grouping_cols=['Month', 'IrRīga'], \n",
    "        aggregation_mode='count', y_col_name='Vakanču skaits', category_col_name='Atrašanās vieta'\n",
    "    )\n",
    "    if not location_data.empty:\n",
    "        block = ReportBlock(\n",
    "            usable_width=USABLE_WIDTH,\n",
    "            block_title=\"Vakanču sadalījums: Rīga vs. reģioni (pēd. 3 mēn.)\",\n",
    "            block_title_style=chart_title_style\n",
    "        )\n",
    "        elements = block.render_line_chart(\n",
    "            chart_data=location_data, x_col='Mēnesis', y_col='Vakanču skaits', category_col='Atrašanās vieta',\n",
    "            selected_company=None, highlight_color=HIGHLIGHT_COLOR, color_palette=COLOR_PALETTE,\n",
    "            x_axis_title='Mēnesis', y_axis_title='Kopējais vakanču skaits', show_legend=True, width_percentage=0.8,\n",
    "            aspect_ratio=0.5 # Using the smaller aspect ratio\n",
    "        )\n",
    "        story.extend(elements)\n",
    "\n",
    "        story.append(Paragraph(\n",
    "            \"\"\"\n",
    "            Šis grafiks attēlo vakanču sadalījumu pēc lokācijas — <b>Rīgā un ārpus Rīgas</b> — laika griezumā pa\n",
    "            mēnešiem. Dati skaidri norāda, ka <b>lielākā daļa vakanču šajā nozarē koncentrējas Rīgā,</b> kur darba\n",
    "            iespēju skaits ir būtiski augstāks nekā reģionos. Gan Rīgā, gan ārpus tās vakanču skaits maija un jūnija\n",
    "            laikā nedaudz samazinās, kas var būt saistāms ar sezonālajām izmaiņām, piemēram, pavasara beigu\n",
    "            un vasaras sākuma periodiem, kad darbinieku plūsma tradicionāli kļūst stabilāka, un uzņēmumi\n",
    "            mazāk aktīvi veic personāla atlasi. Tas var būt arī saistīts ar augstāku kandidātu noturību pēc sezonas\n",
    "            pieņemšanām vai pieprasījuma kritumu vasaras sākumā.\n",
    "            \"\"\",\n",
    "            intro_body_style\n",
    "        ))\n",
    "\n",
    "print(\"\\nSection 3.2 has been built with flexible layout and added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "105af3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction for Section 4 has been added to the story.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4: KONKURENTU ANALĪZE\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Add the title for the new section ---\n",
    "story.append(Spacer(1, 24))\n",
    "section_title_4 = \"4. Konkurentu analīze\"\n",
    "story.append(Paragraph(section_title_4, intro_title_style))\n",
    "story.append(Spacer(1, 6))\n",
    "\n",
    "# --- Add the introductory paragraphs ---\n",
    "text_block_4_1 = \"\"\"\n",
    "Turpmākajās sadaļās apkopota <b>Apranga, LPP Latvia, H&M Hennes & Mauritz SIA, Danija un citu\n",
    "konkurentu</b> – piedāvāto vakanču skaita, algas un populārāko priekšrocību analīze. Šie vizuālie dati\n",
    "ļauj objektīvi izvērtēt gan darba devēju priekšrocību piedāvājumu, gan kandidātu iespējas dažādos\n",
    "Latvijas reģionos.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_block_4_1, intro_body_style))\n",
    "\n",
    "text_block_4_2 = \"\"\"\n",
    "Šajā sekcijā detalizēti salīdzinātas Apranga galveno konkurentu personāla piesaistes stratēģijas,\n",
    "vakances un piedāvātie nosacījumi. Analīze balstās uz pēdējo 3 mēnešu laikā publicētajām vakancēm,\n",
    "akcentējot gan atalgojuma līmeņus, gan biežāk piedāvātos bonusus, darba grafika elastību un citas\n",
    "priekšrocības, ko darba devēji izmanto kandidātu piesaistei.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_block_4_2, intro_body_style))\n",
    "\n",
    "text_block_4_3 = \"\"\"\n",
    "Papildus tiek izvērtēts, kādi amata veidi un nosaukumi bijuši aktuāli pie katra konkurenta, kā arī,\n",
    "kādas atšķirības vērojamas reģionālā un amata specifikā. Sadaļā iekļauti arī vizuāli salīdzinājumi –\n",
    "atalgojuma, bonusu un priekšrocību tabulas, kas ļauj ātri novērtēt Apranga konkurētspēju darba tirgū\n",
    "attiecīgajās pozīcijās.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_block_4_3, intro_body_style))\n",
    "\n",
    "text_block_4_4 = \"\"\"\n",
    "Šī informācija sniedz vispusīgu ieskatu par galveno konkurentu piedāvājumiem un palīdz identificēt\n",
    "jomas, kurās Apranga var uzlabot vai izcelt savu darba devēja vērtību potenciālo kandidātu acīs.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_block_4_4, intro_body_style))\n",
    "\n",
    "# Add a larger spacer before the content that will follow\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "print(\"Introduction for Section 4 has been added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75a7ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically calculated character limit for wrapping: 15\n",
      "\n",
      "Section 4.1, including corrected location analysis, has been built and added to the story.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.1: VAKANČU SKAITS UN SADALĪJUMS [FINAL LAYOUT FIX]\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Step 1: Prepare Base Data (used by all charts in this section) ---\n",
    "competitor_reg_numbers = df_kompanijas_konkurenti['Competitor registration number'].unique().tolist()\n",
    "all_relevant_reg_numbers = competitor_reg_numbers + [selected_company_reg_num]\n",
    "competitor_jobs_df = df_darbnesis_job[df_darbnesis_job['registration_number'].isin(all_relevant_reg_numbers)].copy()\n",
    "today = pd.to_datetime('today')\n",
    "end_date = pd.Timestamp(today.year, today.month, 1)\n",
    "start_date = end_date - pd.DateOffset(months=3)\n",
    "recent_months_df = df_month_assignments[(pd.to_datetime(df_month_assignments['MonthAssignment']) >= start_date) & (pd.to_datetime(df_month_assignments['MonthAssignment']) < end_date)]\n",
    "active_competitor_jobs = pd.merge(competitor_jobs_df, recent_months_df[['id']], on='id', how='inner')\n",
    "apranga_name = df_alias_list[df_alias_list['registration_number'] == selected_company_reg_num]['official_name'].iloc[0]\n",
    "\n",
    "# --- Add all components directly to the main story for flexible layout ---\n",
    "\n",
    "# Add the main section title\n",
    "story.append(Paragraph(\"4.1. Vakanču skaits un sadalījums pēc pozīcijām\", subsection_title_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Component 1: Bar Chart\n",
    "job_counts_by_reg = active_competitor_jobs.groupby('registration_number')['id'].nunique()\n",
    "job_counts_df = pd.merge(job_counts_by_reg.reset_index(), df_distinct_orgs, on='registration_number', how='left')\n",
    "chart_df = job_counts_df.set_index('official_name')[['id']].rename(columns={'id': 'Vakanču skaits'}).sort_values('Vakanču skaits', ascending=False)\n",
    "if not chart_df.empty:\n",
    "    max_count = chart_df['Vakanču skaits'].iloc[0]\n",
    "    threshold = max_count * 0.03\n",
    "    significant_chart_df = chart_df[chart_df['Vakanču skaits'] >= threshold]\n",
    "    chart_df_prepared = prepare_data_with_highlight(df=significant_chart_df, full_color_palette=COLOR_PALETTE, selected_company=apranga_name, highlight_color=HIGHLIGHT_COLOR)\n",
    "    chart_df_final = wrap_df_index_labels(df=chart_df_prepared, usable_chart_width=int(USABLE_WIDTH), approx_char_pixel_width=4, padding_per_bar=8, min_wrap_limit=15)\n",
    "    \n",
    "    bar_chart_block = ReportBlock(\n",
    "        usable_width=USABLE_WIDTH,\n",
    "        block_title=\"Vakanču skaits pēc uzņēmumiem\",\n",
    "        block_title_style=chart_title_style,\n",
    "    )\n",
    "    story.extend(bar_chart_block.render_bar_chart(chart_data=chart_df_final))\n",
    "\n",
    "# Component 2: Descriptive Text\n",
    "story.append(Paragraph(\n",
    "    \"\"\"\n",
    "    (Grafiks nr. 5) Analizējot vakanto darba vietu skaitu dažādās mazumtirdzniecības nozares\n",
    "    mātes kompānijās Latvijā, redzams, ka uzņēmums Apranga ar 24 vakancēm ierindojas\n",
    "    konkurētspējīgā pozīcijā, atrodoties tuvu piektajai vietai starp vadošajiem darba devējiem\n",
    "    šajā segmentā. Visvairāk vakanču šobrīd piedāvā LPP Latvia (57), kam seko Sportland (33),\n",
    "    Poldma Kaubanduse Aktsiaselts (27) un New Yorker (25). Šie dati norāda uz augstu aktivitāti\n",
    "    darbaspēka piesaistē šajos uzņēmumos, taču Apranga saglabā stabilu pozīciju ar līdzvērtīgu\n",
    "    vakanču skaitu. Pārējie tirgus dalībnieki, tostarp Stockmann, Humana, Lindex un H&M;,\n",
    "    uzrāda nedaudz zemāku rādītāju, savukārt tādi uzņēmumi kā CCC Shoes, Danija, WEEKEND,\n",
    "    Eiropas apavi un SIA Peek & Cloppenburg pašlaik piedāvā ievērojami mazāku vakanto vietu\n",
    "    skaitu. Diagrammā attēlotie dati sniedz skaidru ieskatu par kopējo situāciju\n",
    "    mazumtirdzniecības nodarbinātības tirgū un ļauj Apranga novērtēt savu pozīciju\n",
    "    konkurences kontekstā, kā arī identificēt potenciālās iespējas darbaspēka piesaistes\n",
    "    stratēģijas uzlabošanai.\n",
    "    \"\"\",\n",
    "    intro_body_style\n",
    "))\n",
    "story.append(Spacer(1, 18))\n",
    "\n",
    "# Component 3: Title for Matrix and Ribbon Chart\n",
    "story.append(Paragraph(\"Vakanču skaits pa mēnešiem pēc uzņēmumiem\", chart_title_style))\n",
    "story.append(Spacer(1, 6))\n",
    "\n",
    "# Component 4: Matrix Visual\n",
    "active_competitor_jobs_monthly = pd.merge(\n",
    "    active_competitor_jobs[['id', 'registration_number']],\n",
    "    recent_months_df[['id', 'Month']],\n",
    "    on='id', how='inner'\n",
    ").drop_duplicates()\n",
    "active_competitor_jobs_monthly = pd.merge(active_competitor_jobs_monthly, df_distinct_orgs, on='registration_number', how='left')\n",
    "matrix_block = ReportBlock(usable_width=USABLE_WIDTH)\n",
    "story.extend(matrix_block.render_matrix_table(\n",
    "    chart_data=active_competitor_jobs_monthly,\n",
    "    index_col='official_name',\n",
    "    columns_col='Month',\n",
    "    values_col='id'\n",
    "))\n",
    "\n",
    "# Component 5: Ribbon Visual\n",
    "top_companies = chart_df.head(8).index.tolist()\n",
    "ribbon_df_filtered = active_competitor_jobs_monthly[active_competitor_jobs_monthly['official_name'].isin(top_companies)]\n",
    "ribbon_pivot = ribbon_df_filtered.pivot_table(index='official_name', columns='Month', values='id', aggfunc='nunique').fillna(0).astype(int)\n",
    "month_order = ['April', 'May', 'June']\n",
    "present_months = [m for m in month_order if m in ribbon_pivot.columns]\n",
    "ribbon_pivot = ribbon_pivot[present_months]\n",
    "ribbon_pivot['Total'] = ribbon_pivot.sum(axis=1)\n",
    "ribbon_pivot = ribbon_pivot.sort_values(by='Total', ascending=False).drop(columns='Total')\n",
    "other_colors = [color for color in COLOR_PALETTE if color != HIGHLIGHT_COLOR]\n",
    "random.shuffle(other_colors)\n",
    "color_map = {}\n",
    "color_idx = 0\n",
    "for company in ribbon_pivot.index:\n",
    "    if company == apranga_name:\n",
    "        color_map[company] = HIGHLIGHT_COLOR\n",
    "    else:\n",
    "        color_map[company] = other_colors[color_idx % len(other_colors)]\n",
    "        color_idx += 1\n",
    "ribbon_block = ReportBlock(usable_width=USABLE_WIDTH)\n",
    "story.extend(ribbon_block.render_ribbon_chart(chart_data=ribbon_pivot, colors=color_map))\n",
    "\n",
    "# Component 6: Ribbon Chart Explanatory Text\n",
    "story.append(Paragraph(\"(Grafiks nr. 6)\", intro_body_style))\n",
    "story.append(Spacer(1, 18))\n",
    "story.append(Paragraph(\"Kā lasīt grafiku:\", sub_heading_style))\n",
    "bullets = [\"Katra krāsainā josla (lente) attēlo vakanču skaitu konkrētam uzņēmumam katrā mēnesī.\", \"Grafiks ļauj ērti izsekot, kā mainās kopējais vakanču apjoms un kā katra uzņēmuma daļa tajā svārstās laika gaitā.\"]\n",
    "for point in bullets:\n",
    "    story.append(Paragraph(f\"• {point}\", bullet_style))\n",
    "story.append(Spacer(1, 12))\n",
    "story.append(Paragraph(\"Šis grafiks ilustrē vakanču skaita izmaiņas dažādās uzņēmumu grupās trīs mēnešu periodā - 2025. gada aprīlī, maijā un jūnijā. LPP Latvia (rozā krāsa) ir ar lielāko vakanču skaitu katrā mēnesī, taču šis skaits samazinās no 41 aprīlī līdz 37 maijā un 25 jūnijā, kas norāda uz sezonālu svārstību vai samazinātu pieprasījumu vasarā. Sportland (zils), Poldma (sarkans) un Apranga (tumši zils) saglabā salīdzinoši stabilu vakanču skaitu visā periodā, kas liecina par līdzsvarotu pieprasījumu šajos uzņēmumos. Citu uzņēmumu, piemēram, Stockmann, Humana, Lindex un New Yorker, vakanču skaits ir zemāks, taču arī šeit vērojama līdzīga tendence - maza svārstība vai neliels kritums vasaras sākumā.\", intro_body_style))\n",
    "story.append(Spacer(1, 18))\n",
    "\n",
    "# --- [THE FIX] Component 7: Side-by-Side Location Comparison Charts ---\n",
    "def create_simple_bar_image(chart_df, width, height):\n",
    "    \"\"\"A simplified local function to create a bar chart image, avoiding ReportBlock.\"\"\"\n",
    "    max_val = chart_df[chart_df.columns[0]].max()\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=chart_df.index, y=chart_df[chart_df.columns[0]],\n",
    "        marker=dict(color=chart_df['barcolor'], cornerradius=10),\n",
    "        texttemplate='<b>%{y}</b>', textposition='outside',\n",
    "        outsidetextfont=dict(family='Ubuntu', size=24, color='#404040'),\n",
    "        cliponaxis=False\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        font=dict(family=\"Ubuntu\", color=\"#404040\"), plot_bgcolor='white', paper_bgcolor='white',\n",
    "        showlegend=False, margin=dict(l=20, r=20, t=5, b=20),\n",
    "        yaxis=dict(showgrid=False, showticklabels=False, range=[0, max_val * 1.4])\n",
    "    )\n",
    "    img_bytes = fig.to_image(format=\"png\", width=width*3, height=height*3, scale=3)\n",
    "    return Image(BytesIO(img_bytes), width=width, height=height)\n",
    "\n",
    "# Prepare data for both charts\n",
    "df_apranga = active_competitor_jobs[active_competitor_jobs['registration_number'] == selected_company_reg_num]\n",
    "df_competitors = active_competitor_jobs[active_competitor_jobs['registration_number'] != selected_company_reg_num]\n",
    "apranga_counts = df_apranga.groupby('IrRīga')['id'].nunique()\n",
    "competitor_counts = df_competitors.groupby('IrRīga')['id'].nunique()\n",
    "riga_data = pd.DataFrame({'Vakanču skaits': [apranga_counts.get('Rīga', 0), competitor_counts.get('Rīga', 0)]}, index=[apranga_name, 'Konkurenti'])\n",
    "riga_data_prepared = prepare_data_with_highlight(riga_data, [COLOR_PALETTE[0]], apranga_name, HIGHLIGHT_COLOR)\n",
    "arpus_rigas_data = pd.DataFrame({'Vakanču skaits': [apranga_counts.get('Ārpus Rīgas', 0), competitor_counts.get('Ārpus Rīgas', 0)]}, index=[apranga_name, 'Konkurenti'])\n",
    "arpus_rigas_data_prepared = prepare_data_with_highlight(arpus_rigas_data, [COLOR_PALETTE[0]], apranga_name, HIGHLIGHT_COLOR)\n",
    "\n",
    "# Create the chart images and titles\n",
    "chart_width = (USABLE_WIDTH / 2) * 0.9\n",
    "chart_height = chart_width * 0.8\n",
    "image_riga = create_simple_bar_image(riga_data_prepared, chart_width, chart_height)\n",
    "image_arpus = create_simple_bar_image(arpus_rigas_data_prepared, chart_width, chart_height)\n",
    "title_riga = Paragraph(\"Vakances Rīgā\", chart_title_style)\n",
    "title_arpus = Paragraph(\"Vakances ārpus Rīgas\", chart_title_style)\n",
    "\n",
    "# A table cell can safely contain a list of simple flowables.\n",
    "cell_content_riga = [title_riga, Spacer(1, 6), image_riga]\n",
    "cell_content_arpus = [title_arpus, Spacer(1, 6), image_arpus]\n",
    "\n",
    "# Place the lists of flowables into the table\n",
    "location_chart_table = Table([[cell_content_riga, cell_content_arpus]], colWidths=[USABLE_WIDTH/2, USABLE_WIDTH/2])\n",
    "location_chart_table.setStyle(TableStyle([('VALIGN', (0,0), (-1,-1), 'TOP')]))\n",
    "story.append(location_chart_table)\n",
    "\n",
    "# Component 8: Location Chart Explanatory Text\n",
    "story.append(Paragraph(\"(Grafiks nr. 7)\", intro_body_style))\n",
    "story.append(Spacer(1, 12))\n",
    "story.append(Paragraph(\"Analizējot vakanču ģeogrāfisko sadalījumu, redzams, ka <b>Apranga</b> lielāko daļu darba iespēju koncentrē <b>Rīgā, kur šobrīd pieejamas 23 vakances,</b> savukārt <b>ārpus Rīgas</b> norādītas tikai <b>2 vakances.</b> Salīdzinājumam — <b>konkurējošie uzņēmumi</b> Rīgā piedāvā <b>192</b> vakances, bet ārpus tās vēl <b>50,</b> kas norāda uz plašāku darbības pārklājumu arī reģionos.\", intro_body_style))\n",
    "story.append(Paragraph(\"Šī atšķirība tomēr jāskata kontekstā ar uzņēmuma darbības struktūru — <b>Aprangai Latvijā šobrīd ir tikai viens veikals ārpus Rīgas, kas atrodas Jūrmalā.</b> Līdz ar to zemo vakanču skaitu ārpus galvaspilsētas loģiski nosaka arī ierobežotais veikalu skaits šajās teritorijās.\", intro_body_style))\n",
    "story.append(Paragraph(\"Dati kopumā atspoguļo galvaspilsētas orientētu darbaspēka piesaistes modeli, kas šobrīd ir pilnībā atbilstošs esošajai tirdzniecības tīkla struktūrai, taču vienlaikus var <b>kalpot</b> kā bāze turpmākai <b>diskusijai</b> par potenciālu <b>izaugsmi</b> arī reģionālajos centros.\", intro_body_style))\n",
    "\n",
    "print(\"\\nSection 4.1, including corrected location analysis, has been built and added to the story.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d068d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ISCO code analysis (pie chart) has been corrected and added to Section 4.1.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.1 (CONTINUED): VAKANČU SKAITS PĒC POZĪCIJAS (ISCO KODI) [CORRECTED]\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Add all components directly to the story for flexible layout ---\n",
    "\n",
    "# Add the title for this new analysis\n",
    "story.append(Paragraph(\"Vakanču skaits pēc pozīcijas\", chart_title_style))\n",
    "story.append(Spacer(1, 6))\n",
    "\n",
    "# --- Step 1: Prepare data for the Pie Chart ---\n",
    "# Use 'active_competitor_jobs' and get the value counts of the ISCO codes\n",
    "isco_counts = active_competitor_jobs.dropna(subset=['Isco kods'])['Isco kods'].astype(int).astype(str).value_counts()\n",
    "\n",
    "# --- Step 2: Render the Pie Chart (with correct colors) ---\n",
    "pie_chart_block = ReportBlock(\n",
    "    usable_width=USABLE_WIDTH,\n",
    ")\n",
    "# [THE FIX] Passing the COLOR_PALETTE from the config to the render method.\n",
    "story.extend(pie_chart_block.render_pie_chart(\n",
    "    chart_data=isco_counts,\n",
    "    colors=COLOR_PALETTE\n",
    "))\n",
    "\n",
    "# --- Step 3: Add the explanatory text ---\n",
    "story.append(Paragraph(\"(Grafiks nr. 8)\", intro_body_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "text_1 = \"\"\"\n",
    "Sektoru diagrammā redzams, ka pārliecinoši lielākā daļa vakanču <b>(61,94%) atbilst ISCO-08\n",
    "kodam 5223,</b> kas klasificē mazumtirdzniecības pārdevējus. Tas liecina par augstu pieprasījumu\n",
    "pēc pārdošanas darbiniekiem un <b>uzsver</b> šo pozīciju kā dominējošo lomu\n",
    "mazumtirdzniecības sektorā.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_1, intro_body_style))\n",
    "\n",
    "text_2 = \"\"\"\n",
    "Otrā lielākā daļa – <b>13,77%</b> – ir saistīta ar kodu\n",
    "<b>1420,</b> kas attiecas uz mazumtirdzniecības veikalu vadītājiem. Tam seko vairākas citas\n",
    "pozīcijas ar salīdzinoši <b>nelielu</b> īpatsvaru,\n",
    "piemēram, <b>5221 (3,64%), 5222 (3,64%), 1221\n",
    "(2,02%), 1222 (2,02%)</b> un citas. Šie dati liecina\n",
    "par daudzveidīgu, bet koncentrētu\n",
    "pieprasījumu – dominējošais vairākums\n",
    "vakanču ir operatīva līmeņa <b>amatos</b> ar tiešu kontaktu ar klientu.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_2, intro_body_style))\n",
    "\n",
    "text_3 = \"\"\"\n",
    "Jāatzīmē, ka pārējās pozīcijas, piemēram, <b>administratīvie darbinieki, speciālisti un vadības līmeņa\n",
    "darbinieki,</b> katra veido mazāk nekā 5% no kopējā vakanču apjoma, kas norāda uz ierobežotu\n",
    "vajadzību pēc šiem amatiem vai to retāku norādīšanu darba sludinājumos.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_3, intro_body_style))\n",
    "\n",
    "text_4 = \"\"\"\n",
    "Šāds sadalījums sniedz skaidru <b>ieskatu</b> tajā, ka mazumtirdzniecības nozare šobrīd koncentrējas\n",
    "galvenokārt uz darbinieku <b>piesaisti</b> pirmās līnijas pārdošanas pozīcijām, savukārt <b>citu</b> lomu\n",
    "pieejamība ir daudz ierobežotāka.\n",
    "\"\"\"\n",
    "story.append(Paragraph(text_4, intro_body_style))\n",
    "\n",
    "print(\"\\nISCO code analysis (pie chart) has been corrected and added to Section 4.1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09c2d8",
   "metadata": {},
   "source": [
    "## we print the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d178e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report '../Report builder/Reports/report_from_notebook.pdf' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# 3. Generate the document using SimpleDocTemplate\n",
    "# It works perfectly now that our images are sized correctly for its frame.\n",
    "doc = SimpleDocTemplate(\n",
    "    OUTPUT_PDF_PATH,\n",
    "    pagesize=letter,\n",
    "    leftMargin=MARGIN,\n",
    "    rightMargin=MARGIN,\n",
    "    topMargin=MARGIN,\n",
    "    bottomMargin=MARGIN\n",
    ")\n",
    "doc.build(story)\n",
    "\n",
    "print(f\"Report '{OUTPUT_PDF_PATH}' generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
